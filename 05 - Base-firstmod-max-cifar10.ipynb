{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiOATeUxIq_X"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "import numpy as np\n",
        "from skimage.color import rgb2lab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "i_wXo-CEI58D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert RGB images to L, a, and b channels in Lab color space\n",
        "def rgb_to_lab(images):\n",
        "    l_channels = []\n",
        "    ab_channels = []\n",
        "    for img in images:\n",
        "        img = img.permute(1, 2, 0).cpu().numpy()  # Convert to HWC format and move to CPU\n",
        "        lab_image = rgb2lab(img)  # Convert to CIE-Lab\n",
        "\n",
        "        # Normalize L, a, and b channels\n",
        "        L_channel = lab_image[:, :, 0] / 100.0  # Normalize L channel to [0, 1]\n",
        "        a_channel = (lab_image[:, :, 1] + 128) / 255.0  # Normalize a channel to [0, 1]\n",
        "        b_channel = (lab_image[:, :, 2] + 128) / 255.0  # Normalize b channel to [0, 1]\n",
        "\n",
        "        l_channels.append(L_channel)\n",
        "        ab_channels.append(np.stack((a_channel, b_channel), axis=-1))  # Stack a and b\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    L = torch.tensor(np.stack(l_channels), dtype=torch.float32).unsqueeze(1).to(device)  # (N, 1, H, W)\n",
        "    ab = torch.tensor(np.stack(ab_channels), dtype=torch.float32).permute(0, 3, 1, 2).to(device)  # (N, 2, H, W)\n",
        "    return L, ab"
      ],
      "metadata": {
        "id": "_vpgMlvpI7Ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR10 Dataset Loader\n",
        "def load_cifar10_dataset(batch_size=8, num_workers=2):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # Resize to match ResNet/DenseNet input size\n",
        "        transforms.ToTensor(),  # Convert to tensor and scale to [0, 1]\n",
        "    ])\n",
        "\n",
        "    # Load the CIFAR-10 dataset\n",
        "    train_set = datasets.CIFAR10(root='./data/cifar10', train=True, download=True, transform=transform)\n",
        "    test_set = datasets.CIFAR10(root='./data/cifar10', train=False, download=True, transform=transform)\n",
        "\n",
        "    # Create DataLoaders for train and test sets\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "VQBa5_AmI8fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class ColorizationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ColorizationModel, self).__init__()\n",
        "\n",
        "        # Pre-trained ResNet50 encoder (modified to accept 1 channel input)\n",
        "        self.encoder_resnet = models.resnet50(weights='IMAGENET1K_V1')\n",
        "        self.encoder_resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.encoder_resnet = nn.Sequential(*list(self.encoder_resnet.children())[:-2])  # Remove the fully connected layer\n",
        "\n",
        "        # Pre-trained DenseNet121 encoder (modified to accept 1 channel input)\n",
        "        self.encoder_densenet = models.densenet121(weights='IMAGENET1K_V1')\n",
        "        self.encoder_densenet.features.conv0 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.encoder_densenet = nn.Sequential(*list(self.encoder_densenet.children())[:-1])  # Use all layers except the classifier\n",
        "\n",
        "        # Pooling layer to downsample DenseNet output to 7x7\n",
        "        self.downsample_densenet = nn.AdaptiveAvgPool2d((7, 7))\n",
        "\n",
        "        # 1x1 Convolution to reduce ResNet output to 1024 channels for element-wise maximum\n",
        "        self.resnet_conv1x1 = nn.Conv2d(2048, 1024, kernel_size=1)\n",
        "\n",
        "        # 1x1 Convolution to match DenseNet's output to 1024 channels if needed (keeping symmetry)\n",
        "        self.densenet_conv1x1 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
        "\n",
        "        # Fusion Blocks\n",
        "        self.fusion_block1 = nn.Sequential(\n",
        "            nn.Conv2d(1024, 256, kernel_size=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fusion_block2 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fusion_block3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fusion_block4 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder Blocks (same as in the concatenation and averaging model)\n",
        "        self.decoder_block1 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2)\n",
        "        )\n",
        "\n",
        "        self.decoder_block2 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2)\n",
        "        )\n",
        "\n",
        "        self.decoder_block3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2)\n",
        "        )\n",
        "\n",
        "        self.decoder_block4 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2)\n",
        "        )\n",
        "\n",
        "        self.decoder_block5 = nn.Sequential(\n",
        "            nn.Conv2d(256, 2, kernel_size=3, padding=1),\n",
        "            nn.Tanh(),  # Use Tanh to match output range [-1, 1]\n",
        "            nn.Upsample(scale_factor=2)  # Upsample to 224 x 224\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x_resnet = self.encoder_resnet(x)  # ResNet output\n",
        "        x_resnet = self.resnet_conv1x1(x_resnet)  # Reduce ResNet output to 1024 channels\n",
        "        x_densenet = self.encoder_densenet(x)  # DenseNet output\n",
        "        x_densenet = self.downsample_densenet(x_densenet)  # Downsample DenseNet output\n",
        "        x_densenet = self.densenet_conv1x1(x_densenet)  # Match DenseNet output to 1024 channels\n",
        "\n",
        "        # Fusion by element-wise maximum\n",
        "        fb1_input = torch.max(x_resnet, x_densenet)  # Element-wise maximum fusion instead of averaging\n",
        "        fb1_output = self.fusion_block1(fb1_input)\n",
        "\n",
        "        fb2_input = fb1_output  # Use previous output only\n",
        "        fb2_output = self.fusion_block2(fb2_input)\n",
        "\n",
        "        fb3_input = fb2_output  # Use previous output only\n",
        "        fb3_output = self.fusion_block3(fb3_input)\n",
        "\n",
        "        fb4_input = fb3_output  # Use previous output only\n",
        "        fb4_output = self.fusion_block4(fb4_input)\n",
        "\n",
        "        # Decoder\n",
        "        db1_output = self.decoder_block1(fb4_output)\n",
        "        db2_output = self.decoder_block2(db1_output)\n",
        "        db3_output = self.decoder_block3(db2_output)\n",
        "        db4_output = self.decoder_block4(db3_output)\n",
        "\n",
        "        output = self.decoder_block5(db4_output)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "kncz7bEYI-D-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Training loop\n",
        "def train(model, train_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            images = batch[0].to(device)  # Move images to device\n",
        "\n",
        "            # Convert RGB to L and ab channels\n",
        "            L, ab_target = rgb_to_lab(images)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass: model predicts 'a' and 'b' channels from 'L' channel\n",
        "            ab_pred = model(L)  # The input is now just the L channel\n",
        "\n",
        "            # Compute loss between predicted ab channels and ground truth ab channels\n",
        "            loss = criterion(ab_pred, ab_target)\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Update weights\n",
        "\n",
        "            # Print loss every 500 batches\n",
        "            if batch_idx % 500 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "id": "bDRt_YxMJE9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Load the CIFAR10 dataset\n",
        "    batch_size = 8\n",
        "    train_loader, test_loader = load_cifar10_dataset(batch_size=batch_size)\n",
        "\n",
        "    # Initialize the model\n",
        "    model = ColorizationModel().to(device)  # Move model to GPU if available\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.MSELoss()  # Loss for comparing predicted ab with ground truth ab\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    # Train the model\n",
        "    num_epochs = 2\n",
        "    train(model, train_loader, criterion, optimizer, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVeEAKAHJG6G",
        "outputId": "077fe680-1101-41d2-f968-5625ea0700f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar10/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:05<00:00, 34007102.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar10/cifar-10-python.tar.gz to ./data/cifar10\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 151MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
            "100%|██████████| 30.8M/30.8M [00:00<00:00, 88.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2], Batch [0/6250], Loss: 0.0782\n",
            "Epoch [1/2], Batch [500/6250], Loss: 0.0032\n",
            "Epoch [1/2], Batch [1000/6250], Loss: 0.0032\n",
            "Epoch [1/2], Batch [1500/6250], Loss: 0.0032\n",
            "Epoch [1/2], Batch [2000/6250], Loss: 0.0031\n",
            "Epoch [1/2], Batch [2500/6250], Loss: 0.0034\n",
            "Epoch [1/2], Batch [3000/6250], Loss: 0.0016\n",
            "Epoch [1/2], Batch [3500/6250], Loss: 0.0010\n",
            "Epoch [1/2], Batch [4000/6250], Loss: 0.0019\n",
            "Epoch [1/2], Batch [4500/6250], Loss: 0.0027\n",
            "Epoch [1/2], Batch [5000/6250], Loss: 0.0017\n",
            "Epoch [1/2], Batch [5500/6250], Loss: 0.0016\n",
            "Epoch [1/2], Batch [6000/6250], Loss: 0.0022\n",
            "Epoch [2/2], Batch [0/6250], Loss: 0.0024\n",
            "Epoch [2/2], Batch [500/6250], Loss: 0.0019\n",
            "Epoch [2/2], Batch [1000/6250], Loss: 0.0021\n",
            "Epoch [2/2], Batch [1500/6250], Loss: 0.0028\n",
            "Epoch [2/2], Batch [2000/6250], Loss: 0.0025\n",
            "Epoch [2/2], Batch [2500/6250], Loss: 0.0012\n",
            "Epoch [2/2], Batch [3000/6250], Loss: 0.0029\n",
            "Epoch [2/2], Batch [3500/6250], Loss: 0.0017\n",
            "Epoch [2/2], Batch [4000/6250], Loss: 0.0039\n",
            "Epoch [2/2], Batch [4500/6250], Loss: 0.0042\n",
            "Epoch [2/2], Batch [5000/6250], Loss: 0.0018\n",
            "Epoch [2/2], Batch [5500/6250], Loss: 0.0022\n",
            "Epoch [2/2], Batch [6000/6250], Loss: 0.0012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# After training is complete\n",
        "model_save_path = \"colorization_model_max.pth\"\n",
        "\n",
        "# Save the model's state_dict (recommended way to save models in PyTorch)\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "print(f\"Model saved to {model_save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoQOaHThJJlh",
        "outputId": "5785e2a3-e9cf-4882-97dd-d42ebe0f5b53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to colorization_model_max.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mtczhV8cJNUZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}