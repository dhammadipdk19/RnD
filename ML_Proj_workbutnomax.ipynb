{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSeo1XtSgi29"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "from skimage.color import rgb2lab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "Oh8-EtmJhFSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from skimage.color import rgb2lab\n",
        "\n",
        "# Function to scale images to [0, 255]\n",
        "def scale_to_255(x):\n",
        "    return x * 255\n",
        "\n",
        "def rgb_to_ab(images):\n",
        "    a_channels = []\n",
        "    b_channels = []\n",
        "    for img in images:  # Iterate through each image in the batch\n",
        "        img = img.permute(1, 2, 0).cpu().numpy()  # Change to HWC format and move to CPU\n",
        "        lab_image = rgb2lab(img)  # Convert to CIE-Lab\n",
        "\n",
        "        # Normalize L, a, and b channels\n",
        "        L_channel = lab_image[:, :, 0] / 100.0  # Normalize L channel\n",
        "        a_channel = (lab_image[:, :, 1] + 128) / 255.0  # Normalize a channel\n",
        "        b_channel = (lab_image[:, :, 2] + 128) / 255.0  # Normalize b channel\n",
        "\n",
        "        # Collect normalized channels\n",
        "        a_channels.append(a_channel)\n",
        "        b_channels.append(b_channel)\n",
        "\n",
        "    # Stack the a and b channels and convert to tensors\n",
        "    return (\n",
        "        torch.tensor(np.stack(a_channels), dtype=torch.float32).to(device),  # Move to device\n",
        "        torch.tensor(np.stack(b_channels), dtype=torch.float32).to(device)   # Move to device\n",
        "    )"
      ],
      "metadata": {
        "id": "DhgA0N55hHl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR10 Dataset Loader\n",
        "def load_cifar10_dataset(batch_size=8, num_workers=2):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # Resize to match ResNet/DenseNet input size\n",
        "        transforms.ToTensor(),  # Convert to tensor and scale to [0, 1]\n",
        "        transforms.Lambda(scale_to_255),  # Scale to [0, 255]\n",
        "    ])\n",
        "\n",
        "    # Load the CIFAR-10 dataset\n",
        "    train_set = datasets.CIFAR10(root='./data/cifar10', train=True, download=True, transform=transform)\n",
        "    test_set = datasets.CIFAR10(root='./data/cifar10', train=False, download=True, transform=transform)\n",
        "\n",
        "    # Create DataLoaders for train and test sets\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "CdgDtKPChJFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ColorizationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ColorizationModel, self).__init__()\n",
        "\n",
        "        # Pre-trained ResNet50 encoder\n",
        "        self.encoder_resnet = nn.Sequential(\n",
        "            *list(models.resnet50(weights='IMAGENET1K_V1').children())[:-2]\n",
        "        )\n",
        "\n",
        "        # Pre-trained DenseNet121 encoder\n",
        "        self.encoder_densenet = nn.Sequential(\n",
        "            *list(models.densenet121(weights='IMAGENET1K_V1').children())[:-1]  # Use all layers except the classifier\n",
        "        )\n",
        "\n",
        "        # Pooling layer to downsample DenseNet output to 7x7\n",
        "        self.downsample_densenet = nn.AdaptiveAvgPool2d((7, 7))\n",
        "\n",
        "        # 1x1 convolutions to match the channels\n",
        "        self.resnet_conv1x1 = nn.Conv2d(2048, 1024, kernel_size=1)  # Reduce ResNet output channels from 2048 to 1024\n",
        "        self.densenet_conv1x1 = nn.Conv2d(1024, 1024, kernel_size=1)  # Keep DenseNet output channels at 1024\n",
        "\n",
        "        # Fusion Blocks (adjust input channels after max pooling)\n",
        "        self.fusion_block1 = nn.Sequential(\n",
        "            nn.Conv2d(1024, 256, kernel_size=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fusion_block2 = nn.Sequential(  # Adjust input to 256 instead of 512\n",
        "            nn.Conv2d(256, 256, kernel_size=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fusion_block3 = nn.Sequential(  # Adjust input to 256 instead of 512\n",
        "            nn.Conv2d(256, 256, kernel_size=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fusion_block4 = nn.Sequential(  # Adjust input to 256 instead of 512\n",
        "            nn.Conv2d(256, 256, kernel_size=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder Blocks\n",
        "        self.decoder_block1 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2)\n",
        "        )\n",
        "\n",
        "        self.decoder_block2 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2)\n",
        "        )\n",
        "\n",
        "        self.decoder_block3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2)\n",
        "        )\n",
        "\n",
        "        self.decoder_block4 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2)\n",
        "        )\n",
        "\n",
        "        self.decoder_block5 = nn.Sequential(\n",
        "            nn.Conv2d(256, 2, kernel_size=3, padding=1),\n",
        "            nn.Tanh()  # Use Tanh to match output range [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x_resnet = self.encoder_resnet(x)  # ResNet output\n",
        "        x_densenet = self.encoder_densenet(x)  # DenseNet output\n",
        "        x_densenet = self.downsample_densenet(x_densenet)  # Downsample DenseNet output\n",
        "\n",
        "        # Apply 1x1 convolution to match the channel sizes\n",
        "        x_resnet = self.resnet_conv1x1(x_resnet)\n",
        "        x_densenet = self.densenet_conv1x1(x_densenet)\n",
        "\n",
        "        # Fusion Blocks - (Element-wise Max)\n",
        "        fb1_input = torch.max(x_resnet, x_densenet)  # Element-wise max with matching dimensions (1024 channels)\n",
        "        fb1_output = self.fusion_block1(fb1_input)\n",
        "\n",
        "        fb2_input = torch.max(fb1_output, fb1_output)  # Use previous output only (256 channels)\n",
        "        fb2_output = self.fusion_block2(fb2_input)\n",
        "\n",
        "        fb3_input = torch.max(fb2_output, fb2_output)  # Use previous output only (256 channels)\n",
        "        fb3_output = self.fusion_block3(fb3_input)\n",
        "\n",
        "        fb4_input = torch.max(fb3_output, fb3_output)  # Use previous output only (256 channels)\n",
        "        fb4_output = self.fusion_block4(fb4_input)\n",
        "\n",
        "        # Decoder\n",
        "        db1_output = self.decoder_block1(fb4_output)\n",
        "        db2_output = self.decoder_block2(db1_output)\n",
        "        db3_output = self.decoder_block3(db2_output)\n",
        "        db4_output = self.decoder_block4(db3_output)\n",
        "\n",
        "        output = self.decoder_block5(db4_output)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "gMo3mp8uhKh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_idx, batch in enumerate(train_loader):  # Use enumerate to track batch index\n",
        "            images = batch[0]  # Assuming the first element is the images\n",
        "            images = images / 255.0  # Normalize images to [0, 1]\n",
        "            images = images.to(device)  # Move images to the correct device\n",
        "\n",
        "            # Convert images to 'a' and 'b' channels\n",
        "            a_channel, b_channel = rgb_to_ab(images)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)  # Forward pass\n",
        "\n",
        "            # Combine a and b channels\n",
        "            target = torch.stack((a_channel, b_channel), dim=1)  # Combine a and b channels\n",
        "\n",
        "            # Resize target to match model's output size\n",
        "            target_resized = F.interpolate(target, size=(112, 112), mode='bilinear', align_corners=False)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, target_resized)  # Ensure shapes match\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Update weights\n",
        "\n",
        "            # Print loss every 10 batches\n",
        "            if batch_idx % 500 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}')  # Print loss for monitoring"
      ],
      "metadata": {
        "id": "qGrg_Y6diIM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Load the CIFAR10 dataset\n",
        "    batch_size = 8\n",
        "    train_loader, test_loader = load_cifar10_dataset(batch_size=batch_size)\n",
        "\n",
        "    # Initialize the model\n",
        "    model = ColorizationModel().to(device)  # Move model to GPU if available\n",
        "\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    # Train the model\n",
        "    train(model, train_loader, criterion, optimizer, num_epochs=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBev3D2uiOnO",
        "outputId": "5657f470-6e5e-47f6-ab73-60f47a1436dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/2], Batch [0/6250], Loss: 0.2617\n",
            "Epoch [1/2], Batch [500/6250], Loss: 0.0025\n",
            "Epoch [1/2], Batch [1000/6250], Loss: 0.0038\n",
            "Epoch [1/2], Batch [1500/6250], Loss: 0.0015\n",
            "Epoch [1/2], Batch [2000/6250], Loss: 0.0009\n",
            "Epoch [1/2], Batch [2500/6250], Loss: 0.0018\n",
            "Epoch [1/2], Batch [3000/6250], Loss: 0.0021\n",
            "Epoch [1/2], Batch [3500/6250], Loss: 0.0007\n",
            "Epoch [1/2], Batch [4000/6250], Loss: 0.0012\n",
            "Epoch [1/2], Batch [4500/6250], Loss: 0.0012\n",
            "Epoch [1/2], Batch [5000/6250], Loss: 0.0016\n",
            "Epoch [1/2], Batch [5500/6250], Loss: 0.0006\n",
            "Epoch [1/2], Batch [6000/6250], Loss: 0.0009\n",
            "Epoch [2/2], Batch [0/6250], Loss: 0.0015\n",
            "Epoch [2/2], Batch [500/6250], Loss: 0.0009\n",
            "Epoch [2/2], Batch [1000/6250], Loss: 0.0005\n",
            "Epoch [2/2], Batch [1500/6250], Loss: 0.0002\n",
            "Epoch [2/2], Batch [2000/6250], Loss: 0.0004\n",
            "Epoch [2/2], Batch [2500/6250], Loss: 0.0012\n",
            "Epoch [2/2], Batch [3000/6250], Loss: 0.0008\n",
            "Epoch [2/2], Batch [3500/6250], Loss: 0.0009\n",
            "Epoch [2/2], Batch [4000/6250], Loss: 0.0005\n",
            "Epoch [2/2], Batch [4500/6250], Loss: 0.0007\n",
            "Epoch [2/2], Batch [5000/6250], Loss: 0.0005\n",
            "Epoch [2/2], Batch [5500/6250], Loss: 0.0004\n",
            "Epoch [2/2], Batch [6000/6250], Loss: 0.0005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# After training is complete\n",
        "model_save_path = \"colorization_model_max.pth\"\n",
        "\n",
        "# Save the model's state_dict (recommended way to save models in PyTorch)\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "print(f\"Model saved to {model_save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRD5H1sBiRSX",
        "outputId": "b1b51a4a-0d6b-4bf4-e8ce-3b7f6b85c496"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to colorization_model_max.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from skimage.color import lab2rgb\n",
        "\n",
        "def visualize_test_cases(model, test_loader, num_samples=5):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    samples = 0\n",
        "\n",
        "    with torch.no_grad():  # No need to compute gradients during visualization\n",
        "        for batch in test_loader:\n",
        "            images = batch[0]  # Assuming the first element is the images\n",
        "            images = images / 255.0  # Normalize images to [0, 1]\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Convert images to 'a' and 'b' channels\n",
        "            a_channel, b_channel = rgb_to_ab(images)\n",
        "\n",
        "            # Forward pass through the model\n",
        "            predicted_ab = model(images)\n",
        "\n",
        "            # Rescale 'a' and 'b' channels to their original range\n",
        "            predicted_ab_rescaled = predicted_ab.cpu().numpy() * 128.0\n",
        "\n",
        "            # Loop through each image in the batch and visualize\n",
        "            for i in range(images.size(0)):\n",
        "                if samples >= num_samples:\n",
        "                    return  # Stop after num_samples\n",
        "\n",
        "                # Extract the L channel from the original image\n",
        "                l_channel = images[i].cpu().numpy().transpose(1, 2, 0)[:, :, 0] * 100  # Rescale L to [0, 100]\n",
        "\n",
        "                # Get the actual and predicted 'a' and 'b' channels\n",
        "                true_a = a_channel[i].cpu().numpy()\n",
        "                true_b = b_channel[i].cpu().numpy()\n",
        "                predicted_a = predicted_ab_rescaled[i, 0, :, :]\n",
        "                predicted_b = predicted_ab_rescaled[i, 1, :, :]\n",
        "\n",
        "                # Combine L and predicted 'ab' channels for colorized output\n",
        "                predicted_lab = np.stack((l_channel, predicted_a, predicted_b), axis=-1)\n",
        "                predicted_rgb = lab2rgb(predicted_lab)\n",
        "\n",
        "                # Combine L and true 'ab' channels for actual output\n",
        "                true_lab = np.stack((l_channel, true_a, true_b), axis=-1)\n",
        "                true_rgb = lab2rgb(true_lab)\n",
        "\n",
        "                # Plot the images\n",
        "                plt.figure(figsize=(10, 5))\n",
        "\n",
        "                # Original grayscale image (L channel)\n",
        "                plt.subplot(1, 3, 1)\n",
        "                plt.imshow(l_channel, cmap='gray')\n",
        "                plt.title(\"Grayscale (L channel)\")\n",
        "                plt.axis('off')\n",
        "\n",
        "                # True color image (from true 'ab')\n",
        "                plt.subplot(1, 3, 2)\n",
        "                plt.imshow(true_rgb)\n",
        "                plt.title(\"True Color Image\")\n",
        "                plt.axis('off')\n",
        "\n",
        "                # Predicted color image (from predicted 'ab')\n",
        "                plt.subplot(1, 3, 3)\n",
        "                plt.imshow(predicted_rgb)\n",
        "                plt.title(\"Predicted Color Image\")\n",
        "                plt.axis('off')\n",
        "\n",
        "                plt.show()\n",
        "\n",
        "                samples += 1"
      ],
      "metadata": {
        "id": "YBp9bA0xksCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_test_cases(model, test_loader, num_samples=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "83BnxZEbksE_",
        "outputId": "1850aac7-5839-4464-9458-27ebc30e2dd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "all input arrays must have the same shape",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-23338919c549>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvisualize_test_cases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-5dc266aeeffe>\u001b[0m in \u001b[0;36mvisualize_test_cases\u001b[0;34m(model, test_loader, num_samples)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;31m# Combine L and predicted 'ab' channels for colorized output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mpredicted_lab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_channel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0mpredicted_rgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlab2rgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_lab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all input arrays must have the same shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0mresult_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: all input arrays must have the same shape"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lpips"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxfbs02uksIb",
        "outputId": "1a90fdf3-83a9-4a85-b31b-da3b5c70ed97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lpips\n",
            "  Downloading lpips-0.1.4-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from lpips) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (0.19.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from lpips) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (1.13.1)\n",
            "Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (4.66.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (2024.6.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.2.1->lpips) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.0->lpips) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.0->lpips) (1.3.0)\n",
            "Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lpips\n",
            "Successfully installed lpips-0.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PSNR Calculation\n",
        "def calculate_psnr(original, generated):\n",
        "    psnr_value = peak_signal_noise_ratio(original, generated)\n",
        "    return psnr_value\n",
        "\n",
        "# SSIM Calculation\n",
        "def calculate_ssim(original, generated):\n",
        "    ssim_value = structural_similarity(original, generated, multichannel=True)\n",
        "    return ssim_value\n",
        "\n",
        "# LPIPS (Learned Perceptual Image Patch Similarity)\n",
        "def calculate_lpips(original, generated, lpips_model):\n",
        "    original_tensor = torch.from_numpy(original).permute(2, 0, 1).unsqueeze(0).float()  # Convert to tensor\n",
        "    generated_tensor = torch.from_numpy(generated).permute(2, 0, 1).unsqueeze(0).float()\n",
        "    lpips_value = lpips_model(original_tensor, generated_tensor)\n",
        "    return lpips_value.item()\n",
        "\n",
        "# MAE Calculation\n",
        "def calculate_mae(true_ab, predicted_ab):\n",
        "    mae_value = torch.nn.functional.l1_loss(torch.from_numpy(true_ab), torch.from_numpy(predicted_ab)).item()\n",
        "    return mae_value\n",
        "\n",
        "# LPIPS Model for Perceptual Similarity\n",
        "lpips_model = LPIPS(net='vgg')  # Load LPIPS model (VGG backbone)\n",
        "\n",
        "# Define Loss Function\n",
        "def colorization_loss(predicted_ab, true_ab):\n",
        "    return torch.nn.MSELoss()(predicted_ab, true_ab)\n",
        "\n",
        "# Test and Evaluation Function\n",
        "def test_and_evaluate(model, test_loader, lpips_model):\n",
        "    \"\"\"\n",
        "    Test the model and evaluate using PSNR, SSIM, LPIPS, and MAE.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    psnr_scores, ssim_scores, lpips_scores, mae_scores = [], [], [], []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculations for testing\n",
        "        for i, batch in enumerate(test_loader):\n",
        "            images = batch[0].to('cuda' if torch.cuda.is_available() else 'cpu')  # Input L-channel\n",
        "            ab_true = batch[1].to('cuda' if torch.cuda.is_available() else 'cpu')  # True ab channels\n",
        "\n",
        "            predicted_ab = model(images).cpu().numpy()  # Predict ab channels\n",
        "            ab_true = ab_true.cpu().numpy()  # Convert true ab channels to numpy\n",
        "            images = images.cpu().numpy()  # Convert input L-channel to numpy\n",
        "\n",
        "            # Convert Lab to RGB for both true and generated images\n",
        "            true_rgb = lab2rgb(images[0, 0], ab_true[0].transpose(1, 2, 0))\n",
        "            generated_rgb = lab2rgb(images[0, 0], predicted_ab[0].transpose(1, 2, 0))\n",
        "\n",
        "            # Calculate metrics\n",
        "            psnr_value = calculate_psnr(true_rgb, generated_rgb)\n",
        "            ssim_value = calculate_ssim(true_rgb, generated_rgb)\n",
        "            lpips_value = calculate_lpips(true_rgb, generated_rgb, lpips_model)\n",
        "            mae_value = calculate_mae(ab_true, predicted_ab)\n",
        "\n",
        "            # Append the scores for each test image\n",
        "            psnr_scores.append(psnr_value)\n",
        "            ssim_scores.append(ssim_value)\n",
        "            lpips_scores.append(lpips_value)\n",
        "            mae_scores.append(mae_value)\n",
        "\n",
        "    # Compute average scores across all test images\n",
        "    avg_psnr = sum(psnr_scores) / len(psnr_scores)\n",
        "    avg_ssim = sum(ssim_scores) / len(ssim_scores)\n",
        "    avg_lpips = sum(lpips_scores) / len(lpips_scores)\n",
        "    avg_mae = sum(mae_scores) / len(mae_scores)\n",
        "\n",
        "    print(f'Average PSNR: {avg_psnr:.4f}')\n",
        "    print(f'Average SSIM: {avg_ssim:.4f}')\n",
        "    print(f'Average LPIPS: {avg_lpips:.4f}')\n",
        "    print(f'Average MAE: {avg_mae:.4f}')\n",
        "\n",
        "# Example usage\n",
        "# Ensure your test_loader is defined with L and ab channels\n",
        "test_and_evaluate(model, test_loader, lpips_model)\n"
      ],
      "metadata": {
        "id": "_bXjey9nkzlM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}