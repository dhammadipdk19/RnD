{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF80tb9mjiWf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "from skimage.color import rgb2lab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "_tXBWHqEjx0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from skimage.color import rgb2lab\n",
        "\n",
        "# Function to scale images to [0, 255]\n",
        "def scale_to_255(x):\n",
        "    return x * 255\n",
        "\n",
        "def rgb_to_ab(images):\n",
        "    a_channels = []\n",
        "    b_channels = []\n",
        "    for img in images:  # Iterate through each image in the batch\n",
        "        img = img.permute(1, 2, 0).cpu().numpy()  # Change to HWC format and move to CPU\n",
        "        lab_image = rgb2lab(img)  # Convert to CIE-Lab\n",
        "\n",
        "        # Normalize L, a, and b channels\n",
        "        L_channel = lab_image[:, :, 0] / 100.0  # Normalize L channel\n",
        "        a_channel = (lab_image[:, :, 1] + 128) / 255.0  # Normalize a channel\n",
        "        b_channel = (lab_image[:, :, 2] + 128) / 255.0  # Normalize b channel\n",
        "\n",
        "        # Collect normalized channels\n",
        "        a_channels.append(a_channel)\n",
        "        b_channels.append(b_channel)\n",
        "\n",
        "    # Stack the a and b channels and convert to tensors\n",
        "    return (\n",
        "        torch.tensor(np.stack(a_channels), dtype=torch.float32).to(device),  # Move to device\n",
        "        torch.tensor(np.stack(b_channels), dtype=torch.float32).to(device)   # Move to device\n",
        "    )"
      ],
      "metadata": {
        "id": "Cyq1-jKcj6QP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR10 Dataset Loader\n",
        "def load_cifar10_dataset(batch_size=8, num_workers=2):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # Resize to match ResNet/DenseNet input size\n",
        "        transforms.ToTensor(),  # Convert to tensor and scale to [0, 1]\n",
        "        transforms.Lambda(scale_to_255),  # Scale to [0, 255]\n",
        "    ])\n",
        "\n",
        "    # Load the CIFAR-10 dataset\n",
        "    train_set = datasets.CIFAR10(root='./data/cifar10', train=True, download=True, transform=transform)\n",
        "    test_set = datasets.CIFAR10(root='./data/cifar10', train=False, download=True, transform=transform)\n",
        "\n",
        "    # Create DataLoaders for train and test sets\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "wUG89ULYj8rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the colorization model (same as before)\n",
        "class ColorizationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ColorizationModel, self).__init__()\n",
        "\n",
        "        # Pre-trained ResNet50 encoder\n",
        "        self.encoder_resnet = nn.Sequential(\n",
        "            *list(models.resnet50(weights='IMAGENET1K_V1').children())[:-2]\n",
        "        )\n",
        "\n",
        "        # Pre-trained DenseNet121 encoder\n",
        "        self.encoder_densenet = nn.Sequential(\n",
        "            *list(models.densenet121(weights='IMAGENET1K_V1').children())[:-1]  # Use all layers except the classifier\n",
        "        )\n",
        "\n",
        "        # Pooling layer to downsample DenseNet output to 7x7\n",
        "        self.downsample_densenet = nn.AdaptiveAvgPool2d((7, 7))\n",
        "\n",
        "        # Fusion Blocks\n",
        "        self.fusion_block1 = nn.Sequential(\n",
        "            nn.Conv2d(2048 + 1024, 256, kernel_size=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fusion_block2 = nn.Sequential(\n",
        "            nn.Conv2d(256 + 256, 256, kernel_size=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fusion_block3 = nn.Sequential(\n",
        "            nn.Conv2d(256 + 256, 256, kernel_size=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fusion_block4 = nn.Sequential(\n",
        "            nn.Conv2d(256 + 256, 256, kernel_size=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder Blocks\n",
        "        self.decoder_block1 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2)\n",
        "        )\n",
        "\n",
        "        self.decoder_block2 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2)\n",
        "        )\n",
        "\n",
        "        self.decoder_block3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2)\n",
        "        )\n",
        "\n",
        "        self.decoder_block4 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2)\n",
        "        )\n",
        "\n",
        "        self.decoder_block5 = nn.Sequential(\n",
        "            nn.Conv2d(256, 2, kernel_size=3, padding=1),\n",
        "            nn.Tanh()  # Use Tanh to match output range [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x_resnet = self.encoder_resnet(x)  # ResNet output\n",
        "        x_densenet = self.encoder_densenet(x)  # DenseNet output\n",
        "        x_densenet = self.downsample_densenet(x_densenet)  # Downsample DenseNet output\n",
        "\n",
        "        # Fusion Blocks\n",
        "        fb1_input = torch.cat([x_resnet, x_densenet], dim=1)  # 2048 + 1024\n",
        "        fb1_output = self.fusion_block1(fb1_input)\n",
        "\n",
        "        fb2_input = torch.cat([fb1_output, fb1_output], dim=1)  # Use previous output only\n",
        "        fb2_output = self.fusion_block2(fb2_input)\n",
        "\n",
        "        fb3_input = torch.cat([fb2_output, fb2_output], dim=1)  # Use previous output only\n",
        "        fb3_output = self.fusion_block3(fb3_input)\n",
        "\n",
        "        fb4_input = torch.cat([fb3_output, fb3_output], dim=1)  # Use previous output only\n",
        "        fb4_output = self.fusion_block4(fb4_input)\n",
        "\n",
        "        # Decoder\n",
        "        db1_output = self.decoder_block1(fb4_output)\n",
        "        db2_output = self.decoder_block2(db1_output)\n",
        "        db3_output = self.decoder_block3(db2_output)\n",
        "        db4_output = self.decoder_block4(db3_output)\n",
        "\n",
        "        output = self.decoder_block5(db4_output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "-Nw3e3rsj_cF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_idx, batch in enumerate(train_loader):  # Use enumerate to track batch index\n",
        "            images = batch[0]  # Assuming the first element is the images\n",
        "            images = images / 255.0  # Normalize images to [0, 1]\n",
        "            images = images.to(device)  # Move images to the correct device\n",
        "\n",
        "            # Convert images to 'a' and 'b' channels\n",
        "            a_channel, b_channel = rgb_to_ab(images)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)  # Forward pass\n",
        "\n",
        "            # Combine a and b channels\n",
        "            target = torch.stack((a_channel, b_channel), dim=1)  # Combine a and b channels\n",
        "\n",
        "            # Resize target to match model's output size\n",
        "            target_resized = F.interpolate(target, size=(112, 112), mode='bilinear', align_corners=False)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, target_resized)  # Ensure shapes match\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Update weights\n",
        "\n",
        "            # Print loss every 10 batches\n",
        "            if batch_idx % 500 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}')  # Print loss for monitoring"
      ],
      "metadata": {
        "id": "Q9mVUPrjkA_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Load the CIFAR10 dataset\n",
        "    batch_size = 8\n",
        "    train_loader, test_loader = load_cifar10_dataset(batch_size=batch_size)\n",
        "\n",
        "    # Initialize the model\n",
        "    model = ColorizationModel().to(device)  # Move model to GPU if available\n",
        "\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    # Train the model\n",
        "    train(model, train_loader, criterion, optimizer, num_epochs=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_lYDUNxkCfi",
        "outputId": "4053b935-8da9-4fe9-db12-1bcd0ba37a28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar10/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 48461458.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar10/cifar-10-python.tar.gz to ./data/cifar10\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 93.4MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
            "100%|██████████| 30.8M/30.8M [00:00<00:00, 74.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2], Batch [0/6250], Loss: 0.1478\n",
            "Epoch [1/2], Batch [500/6250], Loss: 0.0033\n",
            "Epoch [1/2], Batch [1000/6250], Loss: 0.0034\n",
            "Epoch [1/2], Batch [1500/6250], Loss: 0.0020\n",
            "Epoch [1/2], Batch [2000/6250], Loss: 0.0010\n",
            "Epoch [1/2], Batch [2500/6250], Loss: 0.0006\n",
            "Epoch [1/2], Batch [3000/6250], Loss: 0.0010\n",
            "Epoch [1/2], Batch [3500/6250], Loss: 0.0004\n",
            "Epoch [1/2], Batch [4000/6250], Loss: 0.0005\n",
            "Epoch [1/2], Batch [4500/6250], Loss: 0.0008\n",
            "Epoch [1/2], Batch [5000/6250], Loss: 0.0005\n",
            "Epoch [1/2], Batch [5500/6250], Loss: 0.0009\n",
            "Epoch [1/2], Batch [6000/6250], Loss: 0.0007\n",
            "Epoch [2/2], Batch [0/6250], Loss: 0.0005\n",
            "Epoch [2/2], Batch [500/6250], Loss: 0.0011\n",
            "Epoch [2/2], Batch [1000/6250], Loss: 0.0078\n",
            "Epoch [2/2], Batch [1500/6250], Loss: 0.0006\n",
            "Epoch [2/2], Batch [2000/6250], Loss: 0.0008\n",
            "Epoch [2/2], Batch [2500/6250], Loss: 0.0005\n",
            "Epoch [2/2], Batch [3000/6250], Loss: 0.0008\n",
            "Epoch [2/2], Batch [3500/6250], Loss: 0.0009\n",
            "Epoch [2/2], Batch [4000/6250], Loss: 0.0009\n",
            "Epoch [2/2], Batch [4500/6250], Loss: 0.0020\n",
            "Epoch [2/2], Batch [5000/6250], Loss: 0.0006\n",
            "Epoch [2/2], Batch [5500/6250], Loss: 0.0006\n",
            "Epoch [2/2], Batch [6000/6250], Loss: 0.0006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from skimage.color import lab2rgb\n",
        "\n",
        "def visualize_test_cases(model, test_loader, num_samples=5):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    samples = 0\n",
        "\n",
        "    with torch.no_grad():  # No need to compute gradients during visualization\n",
        "        for batch in test_loader:\n",
        "            images = batch[0]  # Assuming the first element is the images\n",
        "            images = images / 255.0  # Normalize images to [0, 1]\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Convert images to 'a' and 'b' channels\n",
        "            a_channel, b_channel = rgb_to_ab(images)\n",
        "\n",
        "            # Forward pass through the model\n",
        "            predicted_ab = model(images)\n",
        "\n",
        "            # Rescale 'a' and 'b' channels to their original range\n",
        "            predicted_ab_rescaled = predicted_ab.cpu().numpy() * 128.0\n",
        "\n",
        "            # Loop through each image in the batch and visualize\n",
        "            for i in range(images.size(0)):\n",
        "                if samples >= num_samples:\n",
        "                    return  # Stop after num_samples\n",
        "\n",
        "                # Extract the L channel from the original image\n",
        "                l_channel = images[i].cpu().numpy().transpose(1, 2, 0)[:, :, 0] * 100  # Rescale L to [0, 100]\n",
        "\n",
        "                # Get the actual and predicted 'a' and 'b' channels\n",
        "                true_a = a_channel[i].cpu().numpy()\n",
        "                true_b = b_channel[i].cpu().numpy()\n",
        "                predicted_a = predicted_ab_rescaled[i, 0, :, :]\n",
        "                predicted_b = predicted_ab_rescaled[i, 1, :, :]\n",
        "\n",
        "                # Combine L and predicted 'ab' channels for colorized output\n",
        "                predicted_lab = np.stack((l_channel, predicted_a, predicted_b), axis=-1)\n",
        "                predicted_rgb = lab2rgb(predicted_lab)\n",
        "\n",
        "                # Combine L and true 'ab' channels for actual output\n",
        "                true_lab = np.stack((l_channel, true_a, true_b), axis=-1)\n",
        "                true_rgb = lab2rgb(true_lab)\n",
        "\n",
        "                # Plot the images\n",
        "                plt.figure(figsize=(10, 5))\n",
        "\n",
        "                # Original grayscale image (L channel)\n",
        "                plt.subplot(1, 3, 1)\n",
        "                plt.imshow(l_channel, cmap='gray')\n",
        "                plt.title(\"Grayscale (L channel)\")\n",
        "                plt.axis('off')\n",
        "\n",
        "                # True color image (from true 'ab')\n",
        "                plt.subplot(1, 3, 2)\n",
        "                plt.imshow(true_rgb)\n",
        "                plt.title(\"True Color Image\")\n",
        "                plt.axis('off')\n",
        "\n",
        "                # Predicted color image (from predicted 'ab')\n",
        "                plt.subplot(1, 3, 3)\n",
        "                plt.imshow(predicted_rgb)\n",
        "                plt.title(\"Predicted Color Image\")\n",
        "                plt.axis('off')\n",
        "\n",
        "                plt.show()\n",
        "\n",
        "                samples += 1"
      ],
      "metadata": {
        "id": "-PZH19qkkI_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sEvzDarTqZnQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}