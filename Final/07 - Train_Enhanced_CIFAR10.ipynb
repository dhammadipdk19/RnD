{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1mEnSOml1jttyM4jiWtrK-MGnKTvA0q_O","authorship_tag":"ABX9TyPb5DkXx6af2VfRgRvA8Pkq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"qBCTOblFRpzF","executionInfo":{"status":"ok","timestamp":1731836808003,"user_tz":-330,"elapsed":4956,"user":{"displayName":"DHAMMADIP MAHENDRA KAMBLE","userId":"18054280921647504852"}}},"outputs":[],"source":["from skimage import color\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import numpy as np\n","import torch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import models\n"]},{"cell_type":"code","source":["import os\n","import glob\n","import numpy as np\n","from PIL import Image\n","from torch.utils.data import Dataset, DataLoader"],"metadata":{"id":"eNwIcZH5R1zp","executionInfo":{"status":"ok","timestamp":1731836808004,"user_tz":-330,"elapsed":11,"user":{"displayName":"DHAMMADIP MAHENDRA KAMBLE","userId":"18054280921647504852"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Dataset class for DIV2k\n","class ColorizationDataset(Dataset):\n","    def __init__(self, image_list):\n","        self.image_list = image_list\n","        self.transform = transforms.Compose([\n","            transforms.Resize((224, 224)),        # Resize to 224x224\n","            transforms.ToTensor()                 # Convert to Tensor\n","        ])\n","\n","    def __len__(self):\n","        return len(self.image_list)\n","\n","\n","\n","    def __getitem__(self, idx):\n","        # Load image from file path and ensure it's in RGB format\n","        img = Image.open(self.image_list[idx]).convert(\"RGB\")\n","\n","        # Apply the transformation\n","        img = self.transform(img)\n","\n","        # Convert the resized RGB image to Lab color space\n","        img_lab = color.rgb2lab(img.permute(1, 2, 0).numpy()).astype(np.float32)\n","\n","        # Normalize L channel to [-1, 1] and ab channels to [-1, 1]\n","        img_lab[:, :, 0] = img_lab[:, :, 0] / 50.0 - 1  # Normalize L channel to [-1, 1]\n","        img_lab[:, :, 1:] = img_lab[:, :, 1:] / 128.0  # Normalize a and b channels to [-1, 1]\n","\n","        # Separate L and ab channels\n","        L = img_lab[:, :, 0:1]  # Input: L channel\n","        ab = img_lab[:, :, 1:]  # Target: ab channels\n","\n","        # Convert to PyTorch tensor\n","        L = torch.from_numpy(L).permute(2, 0, 1)  # HxWx1 -> 1xHxW\n","        ab = torch.from_numpy(ab).permute(2, 0, 1)  # HxWx2 -> 2xHxW\n","\n","        return L, ab\n"],"metadata":{"id":"NgM0ZQPtR12O","executionInfo":{"status":"ok","timestamp":1731836808004,"user_tz":-330,"elapsed":11,"user":{"displayName":"DHAMMADIP MAHENDRA KAMBLE","userId":"18054280921647504852"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import os\n","import random\n","from torch.utils.data import DataLoader\n","\n","def load_data(batch_size):\n","    def get_limited_images(root_path, limit=50):\n","        all_images = []\n","        for subdir, _, files in os.walk(root_path):\n","            # Filter only .png files\n","            png_files = [os.path.join(subdir, f) for f in files if f.endswith(\".png\")]\n","            # Randomly select 'limit' images from each subfolder\n","            all_images.extend(random.sample(png_files, min(len(png_files), limit)))\n","        return all_images\n","\n","    train_path = \"/content/drive/MyDrive/Colab Notebooks/Machine Learning Lab/Project/CIFAR10_train\"\n","    test_path = \"/content/drive/MyDrive/Colab Notebooks/Machine Learning Lab/Project/CIFAR10_test\"\n","\n","    # Get limited number of image file paths from train and test directories\n","    train_images = get_limited_images(train_path, limit=50)\n","    test_images = get_limited_images(test_path, limit=50)\n","\n","    # Check if images were collected successfully\n","    print(f\"Number of training images: {len(train_images)}\")\n","    print(f\"Number of testing images: {len(test_images)}\")\n","\n","    # Create custom ColorizationDataset\n","    train_data = ColorizationDataset(train_images)\n","    test_data = ColorizationDataset(test_images)\n","\n","    # Create DataLoaders\n","    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","    return train_loader, test_loader\n"],"metadata":{"id":"XBnINvfUR14o","executionInfo":{"status":"ok","timestamp":1731836808004,"user_tz":-330,"elapsed":10,"user":{"displayName":"DHAMMADIP MAHENDRA KAMBLE","userId":"18054280921647504852"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["\n","# Fusion block to combine features from ResNet and DenseNet\n","class FusionBlock(nn.Module):\n","    def __init__(self, in_channels_1, in_channels_2):\n","        super(FusionBlock, self).__init__()\n","        # 1x1 convolution to unify the channel size to 256 for both feature maps\n","        self.conv1 = nn.Conv2d(in_channels_1, 256, kernel_size=1)\n","        self.conv2 = nn.Conv2d(in_channels_2, 256, kernel_size=1)\n","        # Another 1x1 convolution to reduce the concatenated output back to 256 channels\n","        self.reduce_channels = nn.Conv2d(512, 256, kernel_size=1)\n","\n","    def forward(self, x1, x2):\n","        # print(f'FusionBlock - Input x1 shape: {x1.shape}, Input x2 shape: {x2.shape}')\n","        x1 = self.conv1(x1)\n","        x2 = self.conv2(x2)\n","        # Concatenate the two feature maps along the channel dimension\n","        x = torch.cat([x1, x2], dim=1)\n","        # print(f'FusionBlock - After concat shape: {x.shape}')\n","        # Reduce the concatenated output back to 256 channels\n","        x = self.reduce_channels(x)\n","        # print(f'FusionBlock - After reducing channels shape: {x.shape}')\n","        return x\n","\n","# Decoder block with upsampling and unified output to 256 channels\n","class DecoderBlock(nn.Module):\n","    def __init__(self, in_channels=256, out_channels=256):\n","        super(DecoderBlock, self).__init__()\n","        # Expecting 512 channels from the concatenated feature maps, reducing to 256\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n","        self.bn = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n","\n","    def forward(self, x, skip=None):\n","        # print(f'DecoderBlock - Input x shape: {x.shape}')\n","        x = self.conv(x)\n","        x = self.bn(x)\n","        x = self.relu(x)\n","        # print(f'DecoderBlock - After conv shape: {x.shape}')\n","        x = self.upsample(x)\n","        # print(f'DecoderBlock - After upsample shape: {x.shape}')\n","\n","        if skip is not None:\n","            # print(f'DecoderBlock - Skip connection shape: {skip.shape}')\n","            # Upsample skip connection if needed to match spatial size\n","            if skip.shape[2:] != x.shape[2:]:\n","                skip = F.interpolate(skip, size=x.shape[2:], mode='bilinear', align_corners=False)\n","                # print(f'DecoderBlock - After skip upsample shape: {skip.shape}')\n","            x = x + skip\n","            # print(f'DecoderBlock - After adding skip shape: {x.shape}')\n","\n","        return x"],"metadata":{"id":"7_P_YEVUR17K","executionInfo":{"status":"ok","timestamp":1731836808004,"user_tz":-330,"elapsed":10,"user":{"displayName":"DHAMMADIP MAHENDRA KAMBLE","userId":"18054280921647504852"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision.models.feature_extraction import create_feature_extractor\n","from torchvision import models\n","\n"],"metadata":{"id":"Eg7w0y3zR19q","executionInfo":{"status":"ok","timestamp":1731836808004,"user_tz":-330,"elapsed":10,"user":{"displayName":"DHAMMADIP MAHENDRA KAMBLE","userId":"18054280921647504852"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["\n","# Colorization Model using ResNet50 and DenseNet121\n","class ColorizationModel(nn.Module):\n","    def __init__(self):\n","        super(ColorizationModel, self).__init__()\n","\n","        # Pretrained ResNet50 and DenseNet121 as encoders\n","        self.resnet = models.resnet50(pretrained=True)\n","        self.densenet = models.densenet121(pretrained=True)\n","\n","        # Define the layers from which you want to extract features\n","        return_nodes_resnet = {\n","            'layer1': 'resnet_feats_56',   # Feature map size 56x56\n","            'layer2': 'resnet_feats_28',   # Feature map size 28x28\n","            'layer3': 'resnet_feats_14',   # Feature map size 14x14\n","            'layer4': 'resnet_feats_7'     # Feature map size 7x7\n","        }\n","\n","        return_nodes_densenet = {\n","            'features.denseblock1': 'densenet_feats_56',   # Feature map size 56x56\n","            'features.denseblock2': 'densenet_feats_28',   # Feature map size 28x28\n","            'features.denseblock3': 'densenet_feats_14',   # Feature map size 14x14\n","            'features.denseblock4': 'densenet_feats_7'     # Feature map size 7x7\n","        }\n","\n","        # Create feature extractors\n","        self.resnet_extractor = create_feature_extractor(self.resnet, return_nodes=return_nodes_resnet)\n","        self.densenet_extractor = create_feature_extractor(self.densenet, return_nodes=return_nodes_densenet)\n","\n","        # Fusion blocks for multi-level features (each output after concatenation is 512 channels)\n","        self.fusion_56 = FusionBlock(256, 256)  # Concatenate to get 512 channels\n","        self.fusion_28 = FusionBlock(512, 512)  # Concatenate to get 512 channels\n","        self.fusion_14 = FusionBlock(1024, 1024)  # Concatenate to get 512 channels\n","        self.fusion_7 = FusionBlock(2048, 1024)  # Concatenate to get 512 channels\n","\n","        # Decoder blocks with upsampling\n","        self.decoder_7 = DecoderBlock(256)   # Input 512 from fusion_7\n","        self.decoder_14 = DecoderBlock(256)  # Input 512 from fusion_14\n","        self.decoder_28 = DecoderBlock(256)  # Input 512 from fusion_28\n","        self.decoder_56 = DecoderBlock(256)  # Input 512 from fusion_56\n","\n","        # Final output layer (predict ab channels)\n","        self.final_conv = nn.Conv2d(256, 2, kernel_size=3, padding=1)\n","        self.upsample_final = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n","\n","    def forward(self, x):\n","        # Replicate grayscale input to 3 channels for ResNet and DenseNet\n","        x_rgb = x.repeat(1, 3, 1, 1)\n","        # print(f'Forward Pass - Initial input shape: {x_rgb.shape}')\n","\n","        # Extract features from different stages of ResNet and DenseNet\n","        resnet_feats = self.resnet_extractor(x_rgb)\n","        densenet_feats = self.densenet_extractor(x_rgb)\n","\n","        # Get features for each level\n","        resnet_feats_56 = resnet_feats['resnet_feats_56']\n","        resnet_feats_28 = resnet_feats['resnet_feats_28']\n","        resnet_feats_14 = resnet_feats['resnet_feats_14']\n","        resnet_feats_7 = resnet_feats['resnet_feats_7']\n","\n","        densenet_feats_56 = densenet_feats['densenet_feats_56']\n","        densenet_feats_28 = densenet_feats['densenet_feats_28']\n","        densenet_feats_14 = densenet_feats['densenet_feats_14']\n","        densenet_feats_7 = densenet_feats['densenet_feats_7']\n","\n","        # Fusion of multi-level features\n","        fusion_56 = self.fusion_56(resnet_feats_56, densenet_feats_56)\n","        fusion_28 = self.fusion_28(resnet_feats_28, densenet_feats_28)\n","        fusion_14 = self.fusion_14(resnet_feats_14, densenet_feats_14)\n","        fusion_7 = self.fusion_7(resnet_feats_7, densenet_feats_7)\n","\n","        # Decoder with skip connections and unified channels\n","        decoded_7 = self.decoder_7(fusion_7)            # 7x7 -> 14x14\n","        decoded_14 = self.decoder_14(decoded_7, fusion_14)  # 14x14 -> 28x28\n","        decoded_28 = self.decoder_28(decoded_14, fusion_28)  # 28x28 -> 56x56\n","        decoded_56 = self.decoder_56(decoded_28, fusion_56)  # 56x56 -> Final output\n","\n","        # Final prediction for ab channels\n","        ab_pred = self.final_conv(decoded_56)\n","        ab_pred = self.upsample_final(ab_pred)\n","        # print(f'Forward Pass - Final output shape: {ab_pred.shape}')\n","\n","        return ab_pred\n"],"metadata":{"id":"1duRejMbR2AM","executionInfo":{"status":"ok","timestamp":1731836808004,"user_tz":-330,"elapsed":10,"user":{"displayName":"DHAMMADIP MAHENDRA KAMBLE","userId":"18054280921647504852"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import torch.optim as optim\n","import torch.nn.functional as F\n","from tqdm import tqdm"],"metadata":{"id":"wa54NSCBR2C-","executionInfo":{"status":"ok","timestamp":1731836808004,"user_tz":-330,"elapsed":9,"user":{"displayName":"DHAMMADIP MAHENDRA KAMBLE","userId":"18054280921647504852"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def train_model(model, train_loader, test_loader, num_epochs=2, lr=0.001):\n","    # Define optimizer and loss function\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    criterion = nn.MSELoss()  # Assuming you're using MSE loss for the ab channels\n","\n","    model.train()  # Set model to training mode\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        for i, (L, ab) in enumerate(tqdm(train_loader)):\n","            # Move data to the appropriate device (GPU or CPU)\n","            L = L.to(device)\n","            ab = ab.to(device)\n","\n","            # Forward pass\n","            ab_pred = model(L)\n","\n","            # Compute loss\n","            loss = criterion(ab_pred, ab)\n","\n","            # Backward pass and optimization\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Accumulate loss for reporting\n","            running_loss += loss.item()\n","\n","            # Print the loss every 500 batches\n","            if (i + 1) % 8 == 0:\n","                avg_loss = running_loss / 8\n","                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {avg_loss:.4f}')\n","                running_loss = 0.0\n","\n","        # Validation at the end of each epoch\n","        validate_model(model, test_loader)\n","\n","def validate_model(model, test_loader):\n","    model.eval()  # Set model to evaluation mode\n","    running_loss = 0.0\n","    criterion = nn.MSELoss()\n","\n","    with torch.no_grad():\n","        for L, ab in test_loader:\n","            L = L.to(device)\n","            ab = ab.to(device)\n","\n","            # Forward pass\n","            ab_pred = model(L)\n","\n","            # Compute loss\n","            loss = criterion(ab_pred, ab)\n","            running_loss += loss.item()\n","\n","    avg_loss = running_loss / len(test_loader)\n","    print(f'Validation Loss: {avg_loss:.4f}')\n","    model.train()  # Set model back to training mode after validation\n"],"metadata":{"id":"I7edgp1VR2Fb","executionInfo":{"status":"ok","timestamp":1731836808004,"user_tz":-330,"elapsed":7,"user":{"displayName":"DHAMMADIP MAHENDRA KAMBLE","userId":"18054280921647504852"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # Load CIFAR-10 data\n","    batch_size = 16  # You can adjust this based on your system\n","    train_loader, test_loader = load_data(batch_size)\n","\n","    # Initialize the model\n","    model = ColorizationModel().to(device)\n","\n","    # Train the model\n","    train_model(model, train_loader, test_loader, num_epochs=3, lr=0.001)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Z4mjso0R2I_","executionInfo":{"status":"ok","timestamp":1731837207711,"user_tz":-330,"elapsed":399713,"user":{"displayName":"DHAMMADIP MAHENDRA KAMBLE","userId":"18054280921647504852"}},"outputId":"7f02ba6b-4aab-4eb8-912b-85ac909b0036"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of training images: 500\n","Number of testing images: 500\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"," 25%|██▌       | 8/32 [00:41<02:02,  5.10s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/3], Step [8/32], Loss: 0.8121\n"]},{"output_type":"stream","name":"stderr","text":[" 50%|█████     | 16/32 [01:22<01:14,  4.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/3], Step [16/32], Loss: 0.0358\n"]},{"output_type":"stream","name":"stderr","text":[" 75%|███████▌  | 24/32 [02:04<00:40,  5.08s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/3], Step [24/32], Loss: 0.0242\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 32/32 [02:39<00:00,  4.98s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/3], Step [32/32], Loss: 0.0259\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Validation Loss: 0.2419\n"]},{"output_type":"stream","name":"stderr","text":[" 25%|██▌       | 8/32 [00:05<00:15,  1.55it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [2/3], Step [8/32], Loss: 0.0234\n"]},{"output_type":"stream","name":"stderr","text":[" 50%|█████     | 16/32 [00:10<00:09,  1.62it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [2/3], Step [16/32], Loss: 0.0166\n"]},{"output_type":"stream","name":"stderr","text":[" 75%|███████▌  | 24/32 [00:15<00:05,  1.56it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [2/3], Step [24/32], Loss: 0.0127\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 32/32 [00:19<00:00,  1.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [2/3], Step [32/32], Loss: 0.0118\n","Validation Loss: 0.0280\n"]},{"output_type":"stream","name":"stderr","text":[" 25%|██▌       | 8/32 [00:05<00:14,  1.64it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [3/3], Step [8/32], Loss: 0.0115\n"]},{"output_type":"stream","name":"stderr","text":[" 50%|█████     | 16/32 [00:09<00:09,  1.63it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [3/3], Step [16/32], Loss: 0.0129\n"]},{"output_type":"stream","name":"stderr","text":[" 75%|███████▌  | 24/32 [00:15<00:05,  1.56it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [3/3], Step [24/32], Loss: 0.0127\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 32/32 [00:19<00:00,  1.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [3/3], Step [32/32], Loss: 0.0122\n","Validation Loss: 0.0157\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/Machine Learning Lab/Project/final_intnskip_cat_cifar10_clarity.pth')"],"metadata":{"id":"Xj9MZbMpSfmu","executionInfo":{"status":"ok","timestamp":1731837208669,"user_tz":-330,"elapsed":962,"user":{"displayName":"DHAMMADIP MAHENDRA KAMBLE","userId":"18054280921647504852"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WaX8Ti5Yg6PC","executionInfo":{"status":"ok","timestamp":1731837208670,"user_tz":-330,"elapsed":6,"user":{"displayName":"DHAMMADIP MAHENDRA KAMBLE","userId":"18054280921647504852"}}},"execution_count":11,"outputs":[]}]}