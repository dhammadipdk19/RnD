{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1ak0jgqifiiIs2WjszVDxt4v_YVsB7Y-N","authorship_tag":"ABX9TyMl8zanzxNkQhCnu3oeA/dy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"mvN0rLZDZX_w"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torchvision.models.feature_extraction import create_feature_extractor\n","from torchvision import datasets, transforms, models\n","import numpy as np\n","from skimage.color import rgb2lab\n","from torchvision import models\n","from skimage import color\n","import os\n","import glob\n","import numpy as np\n","from PIL import Image\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"code","source":["# Dataset class for DIV2k\n","class ColorizationDataset(Dataset):\n","    def __init__(self, image_list):\n","        self.image_list = image_list\n","        self.transform = transforms.Compose([\n","            transforms.Resize((224, 224)),        # Resize to 224x224\n","            transforms.ToTensor()                 # Convert to Tensor\n","        ])\n","\n","    def __len__(self):\n","        return len(self.image_list)\n","\n","\n","\n","    def __getitem__(self, idx):\n","        # Load image from file path and ensure it's in RGB format\n","        img = Image.open(self.image_list[idx]).convert(\"RGB\")\n","\n","        # Apply the transformation\n","        img = self.transform(img)\n","\n","        # Convert the resized RGB image to Lab color space\n","        img_lab = color.rgb2lab(img.permute(1, 2, 0).numpy()).astype(np.float32)\n","\n","        # Normalize L channel to [-1, 1] and ab channels to [-1, 1]\n","        img_lab[:, :, 0] = img_lab[:, :, 0] / 50.0 - 1  # Normalize L channel to [-1, 1]\n","        img_lab[:, :, 1:] = img_lab[:, :, 1:] / 128.0  # Normalize a and b channels to [-1, 1]\n","\n","        # Separate L and ab channels\n","        L = img_lab[:, :, 0:1]  # Input: L channel\n","        ab = img_lab[:, :, 1:]  # Target: ab channels\n","\n","        # Convert to PyTorch tensor\n","        L = torch.from_numpy(L).permute(2, 0, 1)  # HxWx1 -> 1xHxW\n","        ab = torch.from_numpy(ab).permute(2, 0, 1)  # HxWx2 -> 2xHxW\n","\n","        return L, ab\n"],"metadata":{"id":"Ka5vCT6NZh6U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_div2k_data(batch_size):\n","    train_path = \"/content/drive/MyDrive/Colab Notebooks/Machine Learning Lab/Project/DIV2K/DIV2K_train_HR/DIV2K_train_HR\"\n","    test_path = \"/content/drive/MyDrive/Colab Notebooks/Machine Learning Lab/Project/DIV2K/DIV2K_valid_HR/DIV2K_valid_HR\"\n","\n","    # Get all image file paths in the train and test directories\n","    train_images = glob.glob(os.path.join(train_path, \"*.png\"))\n","    test_images = glob.glob(os.path.join(test_path, \"*.png\"))\n","\n","    # Create custom ColorizationDataset\n","    train_data = ColorizationDataset(train_images)\n","    test_data = ColorizationDataset(test_images)\n","\n","    # Create DataLoaders\n","    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","    return train_loader, test_loader"],"metadata":{"id":"wC_YE6PFZh8s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Fusion block to combine features from ResNet and DenseNet\n","class FusionBlock(nn.Module):\n","    def __init__(self, in_channels_1, in_channels_2):\n","        super(FusionBlock, self).__init__()\n","        # 1x1 convolution to unify the channel size to 256 for both feature maps\n","        self.conv1 = nn.Conv2d(in_channels_1, 256, kernel_size=1)\n","        self.conv2 = nn.Conv2d(in_channels_2, 256, kernel_size=1)\n","        # Another 1x1 convolution to reduce the concatenated output back to 256 channels\n","        self.reduce_channels = nn.Conv2d(512, 256, kernel_size=1)\n","\n","    def forward(self, x1, x2):\n","        # print(f'FusionBlock - Input x1 shape: {x1.shape}, Input x2 shape: {x2.shape}')\n","        x1 = self.conv1(x1)\n","        x2 = self.conv2(x2)\n","        # Concatenate the two feature maps along the channel dimension\n","        x = torch.cat([x1, x2], dim=1)\n","        # print(f'FusionBlock - After concat shape: {x.shape}')\n","        # Reduce the concatenated output back to 256 channels\n","        x = self.reduce_channels(x)\n","        # print(f'FusionBlock - After reducing channels shape: {x.shape}')\n","        return x\n","\n","# Decoder block with upsampling and unified output to 256 channels\n","class DecoderBlock(nn.Module):\n","    def __init__(self, in_channels=256, out_channels=256):\n","        super(DecoderBlock, self).__init__()\n","        # Expecting 512 channels from the concatenated feature maps, reducing to 256\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n","        self.bn = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n","\n","    def forward(self, x, skip=None):\n","        # print(f'DecoderBlock - Input x shape: {x.shape}')\n","        x = self.conv(x)\n","        x = self.bn(x)\n","        x = self.relu(x)\n","        # print(f'DecoderBlock - After conv shape: {x.shape}')\n","        x = self.upsample(x)\n","        # print(f'DecoderBlock - After upsample shape: {x.shape}')\n","\n","        if skip is not None:\n","            # print(f'DecoderBlock - Skip connection shape: {skip.shape}')\n","            # Upsample skip connection if needed to match spatial size\n","            if skip.shape[2:] != x.shape[2:]:\n","                skip = F.interpolate(skip, size=x.shape[2:], mode='bilinear', align_corners=False)\n","                # print(f'DecoderBlock - After skip upsample shape: {skip.shape}')\n","            x = x + skip\n","            # print(f'DecoderBlock - After adding skip shape: {x.shape}')\n","\n","        return x"],"metadata":{"id":"uCQwgKQPZh-6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Colorization Model using ResNet50 and DenseNet121\n","class ColorizationModel(nn.Module):\n","    def __init__(self):\n","        super(ColorizationModel, self).__init__()\n","\n","        # Pretrained ResNet50 and DenseNet121 as encoders\n","        self.resnet = models.resnet50(pretrained=True)\n","        self.densenet = models.densenet121(pretrained=True)\n","\n","        # Define the layers from which you want to extract features\n","        return_nodes_resnet = {\n","            'layer1': 'resnet_feats_56',   # Feature map size 56x56\n","            'layer2': 'resnet_feats_28',   # Feature map size 28x28\n","            'layer3': 'resnet_feats_14',   # Feature map size 14x14\n","            'layer4': 'resnet_feats_7'     # Feature map size 7x7\n","        }\n","\n","        return_nodes_densenet = {\n","            'features.denseblock1': 'densenet_feats_56',   # Feature map size 56x56\n","            'features.denseblock2': 'densenet_feats_28',   # Feature map size 28x28\n","            'features.denseblock3': 'densenet_feats_14',   # Feature map size 14x14\n","            'features.denseblock4': 'densenet_feats_7'     # Feature map size 7x7\n","        }\n","\n","        # Create feature extractors\n","        self.resnet_extractor = create_feature_extractor(self.resnet, return_nodes=return_nodes_resnet)\n","        self.densenet_extractor = create_feature_extractor(self.densenet, return_nodes=return_nodes_densenet)\n","\n","        # Fusion blocks for multi-level features (each output after concatenation is 512 channels)\n","        self.fusion_56 = FusionBlock(256, 256)  # Concatenate to get 512 channels\n","        self.fusion_28 = FusionBlock(512, 512)  # Concatenate to get 512 channels\n","        self.fusion_14 = FusionBlock(1024, 1024)  # Concatenate to get 512 channels\n","        self.fusion_7 = FusionBlock(2048, 1024)  # Concatenate to get 512 channels\n","\n","        # Decoder blocks with upsampling\n","        self.decoder_7 = DecoderBlock(256)   # Input 512 from fusion_7\n","        self.decoder_14 = DecoderBlock(256)  # Input 512 from fusion_14\n","        self.decoder_28 = DecoderBlock(256)  # Input 512 from fusion_28\n","        self.decoder_56 = DecoderBlock(256)  # Input 512 from fusion_56\n","\n","        # Final output layer (predict ab channels)\n","        self.final_conv = nn.Conv2d(256, 2, kernel_size=3, padding=1)\n","        self.upsample_final = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n","\n","    def forward(self, x):\n","        # Replicate grayscale input to 3 channels for ResNet and DenseNet\n","        x_rgb = x.repeat(1, 3, 1, 1)\n","        # print(f'Forward Pass - Initial input shape: {x_rgb.shape}')\n","\n","        # Extract features from different stages of ResNet and DenseNet\n","        resnet_feats = self.resnet_extractor(x_rgb)\n","        densenet_feats = self.densenet_extractor(x_rgb)\n","\n","        # Get features for each level\n","        resnet_feats_56 = resnet_feats['resnet_feats_56']\n","        resnet_feats_28 = resnet_feats['resnet_feats_28']\n","        resnet_feats_14 = resnet_feats['resnet_feats_14']\n","        resnet_feats_7 = resnet_feats['resnet_feats_7']\n","\n","        densenet_feats_56 = densenet_feats['densenet_feats_56']\n","        densenet_feats_28 = densenet_feats['densenet_feats_28']\n","        densenet_feats_14 = densenet_feats['densenet_feats_14']\n","        densenet_feats_7 = densenet_feats['densenet_feats_7']\n","\n","        # Fusion of multi-level features\n","        fusion_56 = self.fusion_56(resnet_feats_56, densenet_feats_56)\n","        fusion_28 = self.fusion_28(resnet_feats_28, densenet_feats_28)\n","        fusion_14 = self.fusion_14(resnet_feats_14, densenet_feats_14)\n","        fusion_7 = self.fusion_7(resnet_feats_7, densenet_feats_7)\n","\n","        # Decoder with skip connections and unified channels\n","        decoded_7 = self.decoder_7(fusion_7)            # 7x7 -> 14x14\n","        decoded_14 = self.decoder_14(decoded_7, fusion_14)  # 14x14 -> 28x28\n","        decoded_28 = self.decoder_28(decoded_14, fusion_28)  # 28x28 -> 56x56\n","        decoded_56 = self.decoder_56(decoded_28, fusion_56)  # 56x56 -> Final output\n","\n","        # Final prediction for ab channels\n","        ab_pred = self.final_conv(decoded_56)\n","        ab_pred = self.upsample_final(ab_pred)\n","        # print(f'Forward Pass - Final output shape: {ab_pred.shape}')\n","\n","        return ab_pred\n"],"metadata":{"id":"NREXL4A3ZiBa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# If you want to use GPU, ensure the model is moved to the GPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"wKjgMkHMZiDs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install lpips\n","import torch\n","import numpy as np\n","from skimage.color import rgb2lab, lab2rgb\n","from skimage.metrics import peak_signal_noise_ratio as psnr\n","from skimage.metrics import structural_similarity as ssim\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","import lpips\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","\n","# Load the LPIPS model\n","lpips_model = lpips.LPIPS(net='alex').to(device)\n","\n","def lab_to_rgb(L, ab):\n","\n","    L = L.squeeze().cpu().numpy()  # Remove batch and convert to numpy\n","    ab = ab.squeeze().cpu().numpy()  # Remove batch and convert to numpy\n","\n","    lab_image = np.zeros((L.shape[0], L.shape[1], 3))  # Create an empty Lab image\n","    lab_image[:, :, 0] = L * 100  # Denormalize L (range is [0, 100])\n","    lab_image[:, :, 1:] = ab.transpose(1, 2, 0) * 100  # Denormalize ab and transpose to match shape\n","\n","    # Convert Lab image to RGB using skimage's lab2rgb\n","    rgb_image = lab2rgb(lab_image.astype(np.float32))\n","    return np.clip(rgb_image, 0, 1)  # Ensure the values are within valid range\n","\n","\n","# Function to visualize grayscale input, colorized output, and original image\n","def visualize_results(grayscale, colorized, original=None):\n","    fig, ax = plt.subplots(1, 3 if original is not None else 2, figsize=(15, 5))\n","\n","    # Grayscale L channel\n","    ax[0].imshow(grayscale.squeeze(), cmap='gray')\n","    ax[0].set_title(\"Grayscale (L channel)\")\n","    ax[0].axis('off')\n","\n","    # Predicted colorized image\n","    ax[1].imshow(colorized)\n","    ax[1].set_title(\"Predicted Colorized Image\")\n","    ax[1].axis('off')\n","\n","    # Original image if provided\n","    if original is not None:\n","        ax[2].imshow(original)\n","        ax[2].set_title(\"Original Image\")\n","        ax[2].axis('off')\n","\n","    plt.show()\n","\n","# Function to compute PSNR\n","def compute_psnr(true_rgb, pred_rgb):\n","    return psnr(true_rgb, pred_rgb, data_range=1)\n","\n","# Function to compute SSIM\n","def compute_ssim(true_rgb, pred_rgb):\n","\n","    min_dim = min(true_rgb.shape[0], true_rgb.shape[1])\n","    win_size = min(7, min_dim)  # Ensure win_size is not larger than the image size\n","\n","    return ssim(true_rgb, pred_rgb, channel_axis=2, data_range=1, win_size=win_size)\n","\n","\n","# Function to compute LPIPS\n","def compute_lpips(true_rgb, pred_rgb, lpips_model):\n","    true_tensor = torch.from_numpy(true_rgb).permute(2, 0, 1).unsqueeze(0).to(device)  # HxWxC -> 1xCxHxW\n","    pred_tensor = torch.from_numpy(pred_rgb).permute(2, 0, 1).unsqueeze(0).to(device)  # HxWxC -> 1xCxHxW\n","    return lpips_model(true_tensor, pred_tensor).item()\n","\n","# Function to evaluate the model on the test dataset and visualize results\n","def evaluate_model(model, test_loader, lpips_model, device):\n","    model.eval()  # Set model to evaluation mode\n","    mse_values, mae_values, psnr_values, ssim_values, lpips_values = [], [], [], [], []\n","\n","    with torch.no_grad():\n","        for i, (L, ab) in enumerate(tqdm(test_loader)):\n","            # Move data to device\n","            L = L.to(device)\n","            ab = ab.to(device)\n","\n","            # Forward pass to get ab predictions\n","            ab_pred = model(L)\n","\n","            # Convert L and predicted ab channels to RGB\n","            pred_rgb = lab_to_rgb(L[0], ab_pred[0])  # Convert the first image in the batch\n","            true_rgb = lab_to_rgb(L[0], ab[0])       # Convert true ab channels to RGB\n","\n","            # Visualize the grayscale input, predicted colorized image, and original RGB image\n","            visualize_results(L[0].cpu().numpy(), pred_rgb, true_rgb)\n","\n","            # Compute MSE and MAE\n","            mse_values.append(mean_squared_error(true_rgb.flatten(), pred_rgb.flatten()))\n","            mae_values.append(mean_absolute_error(true_rgb.flatten(), pred_rgb.flatten()))\n","\n","            # Compute PSNR and SSIM\n","            psnr_values.append(compute_psnr(true_rgb, pred_rgb))\n","            ssim_values.append(compute_ssim(true_rgb, pred_rgb))\n","\n","            # Compute LPIPS\n","            lpips_values.append(compute_lpips(true_rgb, pred_rgb, lpips_model))\n","\n","            # Break after one image (remove or comment this line if you want to evaluate on the full test set)\n","            # break\n","\n","    # Print the average of all metrics\n","    print(f'MSE: {np.mean(mse_values):.4f}')\n","    print(f'MAE: {np.mean(mae_values):.4f}')\n","    print(f'PSNR: {np.mean(psnr_values):.4f}')\n","    print(f'SSIM: {np.mean(ssim_values):.4f}')\n","    print(f'LPIPS: {np.mean(lpips_values):.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DH_Ap-51ZiGZ","executionInfo":{"status":"ok","timestamp":1731784803330,"user_tz":-330,"elapsed":11178,"user":{"displayName":"DHAMMADIP MAHENDRA KAMBLE","userId":"18054280921647504852"}},"outputId":"5bbc0daa-0624-42d3-94ca-819ec9e94ff2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting lpips\n","  Downloading lpips-0.1.4-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from lpips) (2.5.1+cu121)\n","Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (0.20.1+cu121)\n","Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from lpips) (1.26.4)\n","Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (1.13.1)\n","Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (4.66.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=0.4.0->lpips) (1.3.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.2.1->lpips) (11.0.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.0->lpips) (3.0.2)\n","Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: lpips\n","Successfully installed lpips-0.1.4\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n","100%|██████████| 233M/233M [00:02<00:00, 115MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/lpips/lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n"]}]},{"cell_type":"code","source":["\n","model = ColorizationModel().to(device)\n","model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/Machine Learning Lab/Project/final_intnskip_cat_div2k.pth'))\n","\n","\n","lpips_model = lpips.LPIPS(net='alex').to(device)\n","\n","\n","batch_size = 8\n","_, test_loader = load_div2k_data(batch_size)\n","\n","\n","evaluate_model(model, test_loader, lpips_model, device)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1EaS_07ISdnPTkZDT3101OqCujlnj1KIw"},"id":"ZoieQ01mZiIo","executionInfo":{"status":"ok","timestamp":1731784852440,"user_tz":-330,"elapsed":49116,"user":{"displayName":"DHAMMADIP MAHENDRA KAMBLE","userId":"18054280921647504852"}},"outputId":"d1767865-ee00-4f14-9072-faef52510d75"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"1gTr7-4eZiLE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5X3Fr1tnZiNa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rjR_bVpeZiQz"},"execution_count":null,"outputs":[]}]}