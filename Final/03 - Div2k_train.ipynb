{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOMvW+Zhz5k+IxHuTyZZQxt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pk2_hc_eUwl_","executionInfo":{"status":"ok","timestamp":1731783306597,"user_tz":-330,"elapsed":3198,"user":{"displayName":"DHAMMADIP MAHENDRA KAMBLE","userId":"18054280921647504852"}},"outputId":"b4661707-d0b7-482a-fd6e-5f501d465eff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["from skimage import color\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import numpy as np\n","import torch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import models\n"],"metadata":{"id":"wDZmy__4UxKP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import glob\n","import numpy as np\n","from PIL import Image\n","from torch.utils.data import Dataset, DataLoader"],"metadata":{"id":"UvLzhvmFU4Xb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dataset class for DIV2k\n","class ColorizationDataset(Dataset):\n","    def __init__(self, image_list):\n","        self.image_list = image_list\n","        self.transform = transforms.Compose([\n","            transforms.Resize((224, 224)),        # Resize to 224x224\n","            transforms.ToTensor()                 # Convert to Tensor\n","        ])\n","\n","    def __len__(self):\n","        return len(self.image_list)\n","\n","\n","\n","    def __getitem__(self, idx):\n","        # Load image from file path and ensure it's in RGB format\n","        img = Image.open(self.image_list[idx]).convert(\"RGB\")\n","\n","        # Apply the transformation\n","        img = self.transform(img)\n","\n","        # Convert the resized RGB image to Lab color space\n","        img_lab = color.rgb2lab(img.permute(1, 2, 0).numpy()).astype(np.float32)\n","\n","        # Normalize L channel to [-1, 1] and ab channels to [-1, 1]\n","        img_lab[:, :, 0] = img_lab[:, :, 0] / 50.0 - 1  # Normalize L channel to [-1, 1]\n","        img_lab[:, :, 1:] = img_lab[:, :, 1:] / 128.0  # Normalize a and b channels to [-1, 1]\n","\n","        # Separate L and ab channels\n","        L = img_lab[:, :, 0:1]  # Input: L channel\n","        ab = img_lab[:, :, 1:]  # Target: ab channels\n","\n","        # Convert to PyTorch tensor\n","        L = torch.from_numpy(L).permute(2, 0, 1)  # HxWx1 -> 1xHxW\n","        ab = torch.from_numpy(ab).permute(2, 0, 1)  # HxWx2 -> 2xHxW\n","\n","        return L, ab\n"],"metadata":{"id":"OTgdIEglVFd8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_div2k_data(batch_size):\n","    train_path = \"/content/drive/MyDrive/Colab Notebooks/Machine Learning Lab/Project/DIV2K/DIV2K_train_HR/DIV2K_train_HR\"\n","    test_path = \"/content/drive/MyDrive/Colab Notebooks/Machine Learning Lab/Project/DIV2K/DIV2K_valid_HR/DIV2K_valid_HR\"\n","\n","    # Get all image file paths in the train and test directories\n","    train_images = glob.glob(os.path.join(train_path, \"*.png\"))\n","    test_images = glob.glob(os.path.join(test_path, \"*.png\"))\n","\n","    # Create custom ColorizationDataset\n","    train_data = ColorizationDataset(train_images)\n","    test_data = ColorizationDataset(test_images)\n","\n","    # Create DataLoaders\n","    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","    return train_loader, test_loader"],"metadata":{"id":"V8L3rM0PVHGc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Fusion block to combine features from ResNet and DenseNet\n","class FusionBlock(nn.Module):\n","    def __init__(self, in_channels_1, in_channels_2):\n","        super(FusionBlock, self).__init__()\n","        # 1x1 convolution to unify the channel size to 256 for both feature maps\n","        self.conv1 = nn.Conv2d(in_channels_1, 256, kernel_size=1)\n","        self.conv2 = nn.Conv2d(in_channels_2, 256, kernel_size=1)\n","        # Another 1x1 convolution to reduce the concatenated output back to 256 channels\n","        self.reduce_channels = nn.Conv2d(512, 256, kernel_size=1)\n","\n","    def forward(self, x1, x2):\n","        # print(f'FusionBlock - Input x1 shape: {x1.shape}, Input x2 shape: {x2.shape}')\n","        x1 = self.conv1(x1)\n","        x2 = self.conv2(x2)\n","        # Concatenate the two feature maps along the channel dimension\n","        x = torch.cat([x1, x2], dim=1)\n","        # print(f'FusionBlock - After concat shape: {x.shape}')\n","        # Reduce the concatenated output back to 256 channels\n","        x = self.reduce_channels(x)\n","        # print(f'FusionBlock - After reducing channels shape: {x.shape}')\n","        return x\n","\n","# Decoder block with upsampling and unified output to 256 channels\n","class DecoderBlock(nn.Module):\n","    def __init__(self, in_channels=256, out_channels=256):\n","        super(DecoderBlock, self).__init__()\n","        # Expecting 512 channels from the concatenated feature maps, reducing to 256\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n","        self.bn = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n","\n","    def forward(self, x, skip=None):\n","        # print(f'DecoderBlock - Input x shape: {x.shape}')\n","        x = self.conv(x)\n","        x = self.bn(x)\n","        x = self.relu(x)\n","        # print(f'DecoderBlock - After conv shape: {x.shape}')\n","        x = self.upsample(x)\n","        # print(f'DecoderBlock - After upsample shape: {x.shape}')\n","\n","        if skip is not None:\n","            # print(f'DecoderBlock - Skip connection shape: {skip.shape}')\n","            # Upsample skip connection if needed to match spatial size\n","            if skip.shape[2:] != x.shape[2:]:\n","                skip = F.interpolate(skip, size=x.shape[2:], mode='bilinear', align_corners=False)\n","                # print(f'DecoderBlock - After skip upsample shape: {skip.shape}')\n","            x = x + skip\n","            # print(f'DecoderBlock - After adding skip shape: {x.shape}')\n","\n","        return x"],"metadata":{"id":"5_8N7DlzVSvN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision.models.feature_extraction import create_feature_extractor\n","from torchvision import models\n","\n"],"metadata":{"id":"dy02WQCjVW9H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Colorization Model using ResNet50 and DenseNet121\n","class ColorizationModel(nn.Module):\n","    def __init__(self):\n","        super(ColorizationModel, self).__init__()\n","\n","        # Pretrained ResNet50 and DenseNet121 as encoders\n","        self.resnet = models.resnet50(pretrained=True)\n","        self.densenet = models.densenet121(pretrained=True)\n","\n","        # Define the layers from which you want to extract features\n","        return_nodes_resnet = {\n","            'layer1': 'resnet_feats_56',   # Feature map size 56x56\n","            'layer2': 'resnet_feats_28',   # Feature map size 28x28\n","            'layer3': 'resnet_feats_14',   # Feature map size 14x14\n","            'layer4': 'resnet_feats_7'     # Feature map size 7x7\n","        }\n","\n","        return_nodes_densenet = {\n","            'features.denseblock1': 'densenet_feats_56',   # Feature map size 56x56\n","            'features.denseblock2': 'densenet_feats_28',   # Feature map size 28x28\n","            'features.denseblock3': 'densenet_feats_14',   # Feature map size 14x14\n","            'features.denseblock4': 'densenet_feats_7'     # Feature map size 7x7\n","        }\n","\n","        # Create feature extractors\n","        self.resnet_extractor = create_feature_extractor(self.resnet, return_nodes=return_nodes_resnet)\n","        self.densenet_extractor = create_feature_extractor(self.densenet, return_nodes=return_nodes_densenet)\n","\n","        # Fusion blocks for multi-level features (each output after concatenation is 512 channels)\n","        self.fusion_56 = FusionBlock(256, 256)  # Concatenate to get 512 channels\n","        self.fusion_28 = FusionBlock(512, 512)  # Concatenate to get 512 channels\n","        self.fusion_14 = FusionBlock(1024, 1024)  # Concatenate to get 512 channels\n","        self.fusion_7 = FusionBlock(2048, 1024)  # Concatenate to get 512 channels\n","\n","        # Decoder blocks with upsampling\n","        self.decoder_7 = DecoderBlock(256)   # Input 512 from fusion_7\n","        self.decoder_14 = DecoderBlock(256)  # Input 512 from fusion_14\n","        self.decoder_28 = DecoderBlock(256)  # Input 512 from fusion_28\n","        self.decoder_56 = DecoderBlock(256)  # Input 512 from fusion_56\n","\n","        # Final output layer (predict ab channels)\n","        self.final_conv = nn.Conv2d(256, 2, kernel_size=3, padding=1)\n","        self.upsample_final = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n","\n","    def forward(self, x):\n","        # Replicate grayscale input to 3 channels for ResNet and DenseNet\n","        x_rgb = x.repeat(1, 3, 1, 1)\n","        # print(f'Forward Pass - Initial input shape: {x_rgb.shape}')\n","\n","        # Extract features from different stages of ResNet and DenseNet\n","        resnet_feats = self.resnet_extractor(x_rgb)\n","        densenet_feats = self.densenet_extractor(x_rgb)\n","\n","        # Get features for each level\n","        resnet_feats_56 = resnet_feats['resnet_feats_56']\n","        resnet_feats_28 = resnet_feats['resnet_feats_28']\n","        resnet_feats_14 = resnet_feats['resnet_feats_14']\n","        resnet_feats_7 = resnet_feats['resnet_feats_7']\n","\n","        densenet_feats_56 = densenet_feats['densenet_feats_56']\n","        densenet_feats_28 = densenet_feats['densenet_feats_28']\n","        densenet_feats_14 = densenet_feats['densenet_feats_14']\n","        densenet_feats_7 = densenet_feats['densenet_feats_7']\n","\n","        # Fusion of multi-level features\n","        fusion_56 = self.fusion_56(resnet_feats_56, densenet_feats_56)\n","        fusion_28 = self.fusion_28(resnet_feats_28, densenet_feats_28)\n","        fusion_14 = self.fusion_14(resnet_feats_14, densenet_feats_14)\n","        fusion_7 = self.fusion_7(resnet_feats_7, densenet_feats_7)\n","\n","        # Decoder with skip connections and unified channels\n","        decoded_7 = self.decoder_7(fusion_7)            # 7x7 -> 14x14\n","        decoded_14 = self.decoder_14(decoded_7, fusion_14)  # 14x14 -> 28x28\n","        decoded_28 = self.decoder_28(decoded_14, fusion_28)  # 28x28 -> 56x56\n","        decoded_56 = self.decoder_56(decoded_28, fusion_56)  # 56x56 -> Final output\n","\n","        # Final prediction for ab channels\n","        ab_pred = self.final_conv(decoded_56)\n","        ab_pred = self.upsample_final(ab_pred)\n","        # print(f'Forward Pass - Final output shape: {ab_pred.shape}')\n","\n","        return ab_pred\n"],"metadata":{"id":"AMuycghVVYfK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.optim as optim\n","import torch.nn.functional as F\n","from tqdm import tqdm"],"metadata":{"id":"VLIoklTFVZpC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_model(model, train_loader, test_loader, num_epochs=2, lr=0.001):\n","    # Define optimizer and loss function\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    criterion = nn.MSELoss()  # Assuming you're using MSE loss for the ab channels\n","\n","    model.train()  # Set model to training mode\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        for i, (L, ab) in enumerate(tqdm(train_loader)):\n","            # Move data to the appropriate device (GPU or CPU)\n","            L = L.to(device)\n","            ab = ab.to(device)\n","\n","            # Forward pass\n","            ab_pred = model(L)\n","\n","            # Compute loss\n","            loss = criterion(ab_pred, ab)\n","\n","            # Backward pass and optimization\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Accumulate loss for reporting\n","            running_loss += loss.item()\n","\n","            # Print the loss every 500 batches\n","            if (i + 1) % 20 == 0:\n","                avg_loss = running_loss / 20\n","                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {avg_loss:.4f}')\n","                running_loss = 0.0\n","\n","        # Validation at the end of each epoch\n","        validate_model(model, test_loader)\n","\n","def validate_model(model, test_loader):\n","    model.eval()  # Set model to evaluation mode\n","    running_loss = 0.0\n","    criterion = nn.MSELoss()\n","\n","    with torch.no_grad():\n","        for L, ab in test_loader:\n","            L = L.to(device)\n","            ab = ab.to(device)\n","\n","            # Forward pass\n","            ab_pred = model(L)\n","\n","            # Compute loss\n","            loss = criterion(ab_pred, ab)\n","            running_loss += loss.item()\n","\n","    avg_loss = running_loss / len(test_loader)\n","    print(f'Validation Loss: {avg_loss:.4f}')\n","    model.train()  # Set model back to training mode after validation\n"],"metadata":{"id":"j3tQFfIcVaxa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # Load CIFAR-10 data\n","    batch_size = 8  # You can adjust this based on your system\n","    train_loader, test_loader = load_div2k_data(batch_size)\n","\n","    # Initialize the model\n","    model = ColorizationModel().to(device)\n","\n","    # Train the model\n","    train_model(model, train_loader, test_loader, num_epochs=3, lr=0.001)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bA8S4xlGVl_R","executionInfo":{"status":"ok","timestamp":1731784078967,"user_tz":-330,"elapsed":549601,"user":{"displayName":"DHAMMADIP MAHENDRA KAMBLE","userId":"18054280921647504852"}},"outputId":"c114da6d-bc25-431f-b2b9-1931b3a89014"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","100%|██████████| 97.8M/97.8M [00:00<00:00, 129MB/s]\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n","100%|██████████| 30.8M/30.8M [00:00<00:00, 78.7MB/s]\n"," 20%|██        | 20/100 [01:21<01:48,  1.36s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/3], Step [20/100], Loss: 0.3451\n"]},{"output_type":"stream","name":"stderr","text":[" 40%|████      | 40/100 [01:49<01:14,  1.24s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/3], Step [40/100], Loss: 0.0269\n"]},{"output_type":"stream","name":"stderr","text":[" 60%|██████    | 60/100 [02:17<00:55,  1.38s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/3], Step [60/100], Loss: 0.0212\n"]},{"output_type":"stream","name":"stderr","text":[" 80%|████████  | 80/100 [02:45<00:27,  1.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/3], Step [80/100], Loss: 0.0173\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [03:13<00:00,  1.94s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/3], Step [100/100], Loss: 0.0166\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Validation Loss: 0.0202\n"]},{"output_type":"stream","name":"stderr","text":[" 20%|██        | 20/100 [00:25<01:37,  1.22s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch [2/3], Step [20/100], Loss: 0.0154\n"]},{"output_type":"stream","name":"stderr","text":[" 40%|████      | 40/100 [00:51<01:12,  1.21s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch [2/3], Step [40/100], Loss: 0.0189\n"]},{"output_type":"stream","name":"stderr","text":[" 60%|██████    | 60/100 [01:17<00:52,  1.30s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch [2/3], Step [60/100], Loss: 0.0151\n"]},{"output_type":"stream","name":"stderr","text":[" 80%|████████  | 80/100 [01:43<00:27,  1.38s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch [2/3], Step [80/100], Loss: 0.0168\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [02:09<00:00,  1.29s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch [2/3], Step [100/100], Loss: 0.0162\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Validation Loss: 0.0152\n"]},{"output_type":"stream","name":"stderr","text":[" 20%|██        | 20/100 [00:24<01:39,  1.25s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch [3/3], Step [20/100], Loss: 0.0152\n"]},{"output_type":"stream","name":"stderr","text":[" 40%|████      | 40/100 [00:50<01:10,  1.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch [3/3], Step [40/100], Loss: 0.0151\n"]},{"output_type":"stream","name":"stderr","text":[" 60%|██████    | 60/100 [01:15<00:48,  1.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch [3/3], Step [60/100], Loss: 0.0135\n"]},{"output_type":"stream","name":"stderr","text":[" 80%|████████  | 80/100 [01:42<00:24,  1.24s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch [3/3], Step [80/100], Loss: 0.0145\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [02:07<00:00,  1.28s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch [3/3], Step [100/100], Loss: 0.0154\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Validation Loss: 0.1371\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/Machine Learning Lab/Project/final_intnskip_cat_div2k.pth')"],"metadata":{"id":"6eJTgzyjVsWT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Q2KCkaDjV5MM"},"execution_count":null,"outputs":[]}]}