{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upIUGYvmmWVG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "from skimage.color import rgb2lab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "nvc8IfpMmlVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from skimage.color import rgb2lab\n",
        "\n",
        "# Function to scale images to [0, 255]\n",
        "def scale_to_255(x):\n",
        "    return x * 255\n",
        "\n",
        "def rgb_to_ab(images):\n",
        "    a_channels = []\n",
        "    b_channels = []\n",
        "    for img in images:  # Iterate through each image in the batch\n",
        "        img = img.permute(1, 2, 0).cpu().numpy()  # Change to HWC format and move to CPU\n",
        "        lab_image = rgb2lab(img)  # Convert to CIE-Lab\n",
        "\n",
        "        # Normalize L, a, and b channels\n",
        "        L_channel = lab_image[:, :, 0] / 100.0  # Normalize L channel\n",
        "        a_channel = (lab_image[:, :, 1] + 128) / 255.0  # Normalize a channel\n",
        "        b_channel = (lab_image[:, :, 2] + 128) / 255.0  # Normalize b channel\n",
        "\n",
        "        # Collect normalized channels\n",
        "        a_channels.append(a_channel)\n",
        "        b_channels.append(b_channel)\n",
        "\n",
        "    # Stack the a and b channels and convert to tensors\n",
        "    return (\n",
        "        torch.tensor(np.stack(a_channels), dtype=torch.float32).to(device),  # Move to device\n",
        "        torch.tensor(np.stack(b_channels), dtype=torch.float32).to(device)   # Move to device\n",
        "    )"
      ],
      "metadata": {
        "id": "yR3NvdfPmoi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR10 Dataset Loader\n",
        "def load_cifar10_dataset(batch_size=8, num_workers=2):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # Resize to match ResNet/DenseNet input size\n",
        "        transforms.ToTensor(),  # Convert to tensor and scale to [0, 1]\n",
        "        transforms.Lambda(scale_to_255),  # Scale to [0, 255]\n",
        "    ])\n",
        "\n",
        "    # Load the CIFAR-10 dataset\n",
        "    train_set = datasets.CIFAR10(root='./data/cifar10', train=True, download=True, transform=transform)\n",
        "    test_set = datasets.CIFAR10(root='./data/cifar10', train=False, download=True, transform=transform)\n",
        "\n",
        "    # Create DataLoaders for train and test sets\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "JkYaWoJ4mpy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class ColorizationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ColorizationModel, self).__init__()\n",
        "\n",
        "        # Pre-trained ResNet50 encoder\n",
        "        self.encoder_resnet = nn.Sequential(\n",
        "            *list(models.resnet50(weights='IMAGENET1K_V1').children())[:-2]\n",
        "        )\n",
        "\n",
        "        # Pre-trained DenseNet121 encoder\n",
        "        self.encoder_densenet = nn.Sequential(\n",
        "            *list(models.densenet121(weights='IMAGENET1K_V1').children())[:-1]  # Use all layers except the classifier\n",
        "        )\n",
        "\n",
        "        # Pooling layer to downsample DenseNet output to 7x7\n",
        "        self.downsample_densenet = nn.AdaptiveAvgPool2d((7, 7))\n",
        "\n",
        "        # 1x1 convolutions to match the channels\n",
        "        self.resnet_conv1x1 = nn.Conv2d(2048, 1024, kernel_size=1)  # Reduce ResNet output channels from 2048 to 1024\n",
        "        self.densenet_conv1x1 = nn.Conv2d(1024, 1024, kernel_size=1)  # Keep DenseNet output channels at 1024\n",
        "\n",
        "        # Fusion Blocks (adjust input channels after max pooling)\n",
        "        self.fusion_block1 = nn.Sequential(\n",
        "            nn.Conv2d(1024, 256, kernel_size=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fusion_block2 = nn.Sequential(  # Adjust input to 256 instead of 512\n",
        "            nn.Conv2d(256, 256, kernel_size=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fusion_block3 = nn.Sequential(  # Adjust input to 256 instead of 512\n",
        "            nn.Conv2d(256, 256, kernel_size=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fusion_block4 = nn.Sequential(  # Adjust input to 256 instead of 512\n",
        "            nn.Conv2d(256, 256, kernel_size=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder Blocks\n",
        "        self.decoder_block1 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2)\n",
        "        )\n",
        "\n",
        "        self.decoder_block2 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2)\n",
        "        )\n",
        "\n",
        "        self.decoder_block3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2)\n",
        "        )\n",
        "\n",
        "        self.decoder_block4 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2)\n",
        "        )\n",
        "\n",
        "        self.decoder_block5 = nn.Sequential(\n",
        "            nn.Conv2d(256, 2, kernel_size=3, padding=1),\n",
        "            nn.Tanh()  # Use Tanh to match output range [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x_resnet = self.encoder_resnet(x)  # ResNet output\n",
        "        x_densenet = self.encoder_densenet(x)  # DenseNet output\n",
        "        x_densenet = self.downsample_densenet(x_densenet)  # Downsample DenseNet output\n",
        "\n",
        "        # Apply 1x1 convolution to match the channel sizes\n",
        "        x_resnet = self.resnet_conv1x1(x_resnet)\n",
        "        x_densenet = self.densenet_conv1x1(x_densenet)\n",
        "\n",
        "        # Fusion Blocks - (Element-wise Averaging)\n",
        "        fb1_input = (x_resnet + x_densenet) / 2  # Element-wise averaging (1024 channels)\n",
        "        fb1_output = self.fusion_block1(fb1_input)\n",
        "\n",
        "        fb2_input = (fb1_output + fb1_output) / 2  # Use previous output only (256 channels)\n",
        "        fb2_output = self.fusion_block2(fb2_input)\n",
        "\n",
        "        fb3_input = (fb2_output + fb2_output) / 2  # Use previous output only (256 channels)\n",
        "        fb3_output = self.fusion_block3(fb3_input)\n",
        "\n",
        "        fb4_input = (fb3_output + fb3_output) / 2  # Use previous output only (256 channels)\n",
        "        fb4_output = self.fusion_block4(fb4_input)\n",
        "\n",
        "        # Decoder\n",
        "        db1_output = self.decoder_block1(fb4_output)\n",
        "        db2_output = self.decoder_block2(db1_output)\n",
        "        db3_output = self.decoder_block3(db2_output)\n",
        "        db4_output = self.decoder_block4(db3_output)\n",
        "\n",
        "        output = self.decoder_block5(db4_output)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "RQ7E6GwDmq_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_idx, batch in enumerate(train_loader):  # Use enumerate to track batch index\n",
        "            images = batch[0]  # Assuming the first element is the images\n",
        "            images = images / 255.0  # Normalize images to [0, 1]\n",
        "            images = images.to(device)  # Move images to the correct device\n",
        "\n",
        "            # Convert images to 'a' and 'b' channels\n",
        "            a_channel, b_channel = rgb_to_ab(images)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)  # Forward pass\n",
        "\n",
        "            # Combine a and b channels\n",
        "            target = torch.stack((a_channel, b_channel), dim=1)  # Combine a and b channels\n",
        "\n",
        "            # Resize target to match model's output size\n",
        "            target_resized = F.interpolate(target, size=(112, 112), mode='bilinear', align_corners=False)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, target_resized)  # Ensure shapes match\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Update weights\n",
        "\n",
        "            # Print loss every 10 batches\n",
        "            if batch_idx % 500 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}')  # Print loss for monitoring"
      ],
      "metadata": {
        "id": "9BWrdftgmtog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Load the CIFAR10 dataset\n",
        "    batch_size = 8\n",
        "    train_loader, test_loader = load_cifar10_dataset(batch_size=batch_size)\n",
        "\n",
        "    # Initialize the model\n",
        "    model = ColorizationModel().to(device)  # Move model to GPU if available\n",
        "\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    # Train the model\n",
        "    train(model, train_loader, criterion, optimizer, num_epochs=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUKz_bohmyki",
        "outputId": "8c674816-b627-4dc3-d81e-66f16e84e2a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/2], Batch [0/6250], Loss: 0.3217\n",
            "Epoch [1/2], Batch [500/6250], Loss: 0.0064\n",
            "Epoch [1/2], Batch [1000/6250], Loss: 0.0045\n",
            "Epoch [1/2], Batch [1500/6250], Loss: 0.0063\n",
            "Epoch [1/2], Batch [2000/6250], Loss: 0.0016\n",
            "Epoch [1/2], Batch [2500/6250], Loss: 0.0012\n",
            "Epoch [1/2], Batch [3000/6250], Loss: 0.0022\n",
            "Epoch [1/2], Batch [3500/6250], Loss: 0.0025\n",
            "Epoch [1/2], Batch [4000/6250], Loss: 0.0024\n",
            "Epoch [1/2], Batch [4500/6250], Loss: 0.0004\n",
            "Epoch [1/2], Batch [5000/6250], Loss: 0.0016\n",
            "Epoch [1/2], Batch [5500/6250], Loss: 0.0006\n",
            "Epoch [1/2], Batch [6000/6250], Loss: 0.0022\n",
            "Epoch [2/2], Batch [0/6250], Loss: 0.0010\n",
            "Epoch [2/2], Batch [500/6250], Loss: 0.0006\n",
            "Epoch [2/2], Batch [1000/6250], Loss: 0.0005\n",
            "Epoch [2/2], Batch [1500/6250], Loss: 0.0020\n",
            "Epoch [2/2], Batch [2000/6250], Loss: 0.0010\n",
            "Epoch [2/2], Batch [2500/6250], Loss: 0.0026\n",
            "Epoch [2/2], Batch [3000/6250], Loss: 0.0011\n",
            "Epoch [2/2], Batch [3500/6250], Loss: 0.0015\n",
            "Epoch [2/2], Batch [4000/6250], Loss: 0.0006\n",
            "Epoch [2/2], Batch [4500/6250], Loss: 0.0017\n",
            "Epoch [2/2], Batch [5000/6250], Loss: 0.0009\n",
            "Epoch [2/2], Batch [5500/6250], Loss: 0.0009\n",
            "Epoch [2/2], Batch [6000/6250], Loss: 0.0009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# After training is complete\n",
        "model_save_path = \"colorization_model_avg.pth\"\n",
        "\n",
        "# Save the model's state_dict (recommended way to save models in PyTorch)\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "print(f\"Model saved to {model_save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDx4KbkIm0d3",
        "outputId": "e1fd303b-d446-48fb-8358-65afd4a2c144"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to colorization_model_avg.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jDrl6P4pxbM1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}