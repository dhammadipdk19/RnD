{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJMY4yUCfE66"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Q5SrPftkfNly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "from skimage.color import rgb2lab\n",
        "from skimage.transform import resize\n",
        "\n",
        "# Custom Dataset for CIFAR10\n",
        "class CIFAR10Colorization(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "        self.input_size = 224  # Target size for pre-trained models\n",
        "\n",
        "        # Define the transformation to resize images\n",
        "        self.resize_transform = transforms.Resize((self.input_size, self.input_size))\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the original image from the dataset (RGB format)\n",
        "        img, _ = self.dataset[idx]\n",
        "\n",
        "        # Convert PIL image to NumPy array and resize it to the required size (224x224)\n",
        "        img_resized = np.array(self.resize_transform(img))\n",
        "\n",
        "        # Convert the resized RGB image to CIE Lab color space\n",
        "        lab_img = rgb2lab(img_resized).astype(np.float32)\n",
        "\n",
        "        # Extract the L channel (luminance) and ab channels (chrominance)\n",
        "        L = lab_img[:, :, 0]  # L channel (luminance)\n",
        "        ab = lab_img[:, :, 1:]  # ab channels (chrominance)\n",
        "\n",
        "        # Normalize the L channel to [0, 1]\n",
        "        L = L / 100.0\n",
        "\n",
        "        # Normalize the ab channels to [-1, 1]\n",
        "        ab = (ab + 128) / 255.0 * 2.0 - 1.0\n",
        "\n",
        "        # Convert L and ab to PyTorch tensors\n",
        "        L = torch.from_numpy(L).unsqueeze(0)  # Add channel dimension for L\n",
        "        ab = torch.from_numpy(ab).permute(2, 0, 1)  # Change from HxWx2 to 2xHxW\n",
        "\n",
        "        return L, ab\n",
        "\n",
        "# Load the CIFAR10 dataset\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True)\n",
        "\n",
        "# Create an instance of the CIFAR10Colorization dataset\n",
        "colorization_dataset = CIFAR10Colorization(train_dataset)\n",
        "\n",
        "# Create a DataLoader for batching\n",
        "data_loader = DataLoader(colorization_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Iterate through the DataLoader (example)\n",
        "for L, ab in data_loader:\n",
        "    print(f\"Luminance (L) shape: {L.shape}, Chrominance (ab) shape: {ab.shape}\")\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRMQ4yqFGQQ4",
        "outputId": "53f8c411-0901-4710-f9cd-4e939624d57a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 48927347.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Luminance (L) shape: torch.Size([32, 1, 224, 224]), Chrominance (ab) shape: torch.Size([32, 2, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "# Define the encoder model with max fusion and 1x1 convolutions\n",
        "class EnsembleEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnsembleEncoder, self).__init__()\n",
        "\n",
        "        # Load pre-trained ResNet50 and DenseNet121 without the FC layers\n",
        "        self.resnet50 = models.resnet50(pretrained=True)\n",
        "        self.densenet121 = models.densenet121(pretrained=True)\n",
        "\n",
        "        # Remove the FC layers from ResNet50 and DenseNet121\n",
        "        self.resnet50 = nn.Sequential(*list(self.resnet50.children())[:-2])\n",
        "        self.densenet121 = nn.Sequential(*list(self.densenet121.children())[:-1])\n",
        "\n",
        "        # 1x1 convolution layers to match the number of channels before fusion\n",
        "        self.conv_1x1_resnet = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1),\n",
        "            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1),\n",
        "            nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=1),\n",
        "            nn.Conv2d(in_channels=2048, out_channels=1024, kernel_size=1)\n",
        "        ])\n",
        "\n",
        "        self.conv_1x1_densenet = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1),\n",
        "            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1),\n",
        "            nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=1),\n",
        "            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=1)\n",
        "        ])\n",
        "\n",
        "        # Specify which layers to extract features from (based on feature map sizes)\n",
        "        self.resnet_layers = [3, 4, 5, 6]  # ResNet stages for 56x56, 28x28, 14x14, 7x7 feature maps\n",
        "        self.densenet_layers = [6, 8, 10, 12]  # DenseNet blocks: Block 1, Block 2, Block 3, Block 4\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Replicate grayscale image to 3 channels\n",
        "        x = x.repeat(1, 3, 1, 1)\n",
        "\n",
        "        # ResNet50 forward pass and feature extraction\n",
        "        resnet_features = []\n",
        "        for i, layer in enumerate(self.resnet50):\n",
        "            x = layer(x)\n",
        "            if i in self.resnet_layers:\n",
        "                resnet_features.append(x)\n",
        "\n",
        "        # DenseNet121 forward pass and feature extraction\n",
        "        densenet_features = []\n",
        "        for i, layer in enumerate(self.densenet121):\n",
        "            x = layer(x)\n",
        "            if i in self.densenet_layers:\n",
        "                densenet_features.append(x)\n",
        "\n",
        "        # Apply 1x1 convolutions and fuse the features using max pooling\n",
        "        fused_features = []\n",
        "        for i, (resnet_f, densenet_f) in enumerate(zip(resnet_features, densenet_features)):\n",
        "            # Apply 1x1 convolution to match the channel dimensions\n",
        "            resnet_f_conv = self.conv_1x1_resnet[i](resnet_f)\n",
        "            densenet_f_conv = self.conv_1x1_densenet[i](densenet_f)\n",
        "\n",
        "            # Max fusion of the feature maps at this level\n",
        "            fused_f = torch.max(resnet_f_conv, densenet_f_conv)\n",
        "            fused_features.append(fused_f)\n",
        "\n",
        "        return fused_features\n",
        "\n",
        "# Example usage\n",
        "encoder = EnsembleEncoder()\n",
        "# input_image = torch.randn(1, 1, 224, 224)  # Example grayscale input (batch_size=1, channels=1, H=224, W=224)\n",
        "# fused_features = encoder(input_image)\n",
        "\n",
        "# Check feature sizes\n",
        "# for i, feature in enumerate(fused_features):\n",
        "#     print(f\"Fused feature map {i+1} shape: {feature.shape}\")\n"
      ],
      "metadata": {
        "id": "_EG5EAZJGg-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the decoder network\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # Decoder block 1: Receives 7x7 features from encoder and previous decoder output\n",
        "        self.decoder_block1 = self._make_decoder_block(1024, 512)\n",
        "\n",
        "        # Decoder block 2: Receives 14x14 features from encoder and previous decoder output\n",
        "        self.decoder_block2 = self._make_decoder_block(512, 256)\n",
        "\n",
        "        # Decoder block 3: Receives 28x28 features from encoder and previous decoder output\n",
        "        self.decoder_block3 = self._make_decoder_block(256, 128)\n",
        "\n",
        "        # Decoder block 4: Receives 56x56 features from encoder and previous decoder output\n",
        "        self.decoder_block4 = self._make_decoder_block(128, 64)\n",
        "\n",
        "        # Final output layer to predict ab channels\n",
        "        self.final_conv = nn.Conv2d(64, 2, kernel_size=1)  # 2 channels for ab output (CIE-Lab color space)\n",
        "\n",
        "    def _make_decoder_block(self, in_channels, out_channels):\n",
        "        \"\"\"\n",
        "        Helper function to create a decoder block.\n",
        "        Each block consists of Conv2D, BatchNorm, and Bilinear Upsampling.\n",
        "        \"\"\"\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, fused_features):\n",
        "        \"\"\"\n",
        "        Forward pass of the decoder.\n",
        "        Fused features are a list containing the multi-level fused features from the encoder.\n",
        "        \"\"\"\n",
        "        # Starting from the coarsest features (7x7) and moving up\n",
        "        x = self.decoder_block1(fused_features[-1])  # 7x7 -> 14x14\n",
        "\n",
        "        # Decoder block 2 with skip connection (fused_features[2] is 14x14)\n",
        "        x = torch.cat((x, fused_features[-2]), dim=1)\n",
        "        x = self.decoder_block2(x)  # 14x14 -> 28x28\n",
        "\n",
        "        # Decoder block 3 with skip connection (fused_features[1] is 28x28)\n",
        "        x = torch.cat((x, fused_features[-3]), dim=1)\n",
        "        x = self.decoder_block3(x)  # 28x28 -> 56x56\n",
        "\n",
        "        # Decoder block 4 with skip connection (fused_features[0] is 56x56)\n",
        "        x = torch.cat((x, fused_features[-4]), dim=1)\n",
        "        x = self.decoder_block4(x)  # 56x56 -> 112x112\n",
        "\n",
        "        # Final output layer to predict ab channels\n",
        "        ab_output = self.final_conv(x)\n",
        "\n",
        "        return ab_output\n",
        "\n",
        "# Example Usage\n",
        "decoder = Decoder()\n",
        "# Assuming fused_features from the encoder, which are a list of feature maps\n",
        "# Here we simulate with random tensors for illustration (shape matching feature maps)\n",
        "# fused_features = [\n",
        "#     torch.randn(1, 128, 56, 56),  # 56x56\n",
        "#     torch.randn(1, 256, 28, 28),  # 28x28\n",
        "#     torch.randn(1, 512, 14, 14),  # 14x14\n",
        "#     torch.randn(1, 1024, 7, 7),   # 7x7\n",
        "# ]\n",
        "\n",
        "# # Forward pass through decoder\n",
        "# ab_output = decoder(fused_features)\n",
        "# print(f\"Decoder output shape (ab channels): {ab_output.shape}\")  # Should be (1, 2, 112, 112)\n"
      ],
      "metadata": {
        "id": "uTp_36FtGiQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "import numpy as np\n",
        "from skimage import color\n",
        "\n",
        "# Custom dataset to convert RGB to Lab and process only L as input\n",
        "class CIFAR10_LabColorization(CIFAR10):\n",
        "    def __getitem__(self, index):\n",
        "        img, target = super(CIFAR10_LabColorization, self).__getitem__(index)\n",
        "\n",
        "        # Convert the RGB image to Lab color space\n",
        "        img_lab = color.rgb2lab(np.array(img) / 255.0)  # Normalize to [0, 1] before conversion\n",
        "\n",
        "        # Normalize L channel to range [-1, 1]\n",
        "        L = img_lab[:, :, 0] / 50.0 - 1.0  # L channel range: 0-100 normalized to [-1, 1]\n",
        "\n",
        "        # Normalize ab channels to range [-1, 1]\n",
        "        ab = img_lab[:, :, 1:] / 128.0  # a and b channels range: -128 to 127 normalized to [-1, 1]\n",
        "\n",
        "        # Return L as input (1 channel), ab as target (2 channels)\n",
        "        return torch.FloatTensor(L).unsqueeze(0), torch.FloatTensor(ab).permute(2, 0, 1)\n",
        "\n",
        "# Training configuration\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "num_epochs = 25\n",
        "\n",
        "# Data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to match the input size for the encoder\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Load CIFAR10 dataset (train and test sets)\n",
        "train_dataset = CIFAR10_LabColorization(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = CIFAR10_LabColorization(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize the ensemble encoder and decoder\n",
        "encoder = EnsembleEncoder()\n",
        "decoder = Decoder()\n",
        "\n",
        "# Combine encoder and decoder into a single model\n",
        "class ColorizationModel(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(ColorizationModel, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass the grayscale input through the encoder\n",
        "        fused_features = self.encoder(x)\n",
        "\n",
        "        # Pass the fused features through the decoder to get the ab channels\n",
        "        ab_output = self.decoder(fused_features)\n",
        "\n",
        "        return ab_output\n",
        "\n",
        "# Initialize the complete colorization model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = ColorizationModel(encoder, decoder).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()  # Mean Squared Error (MSE) loss for ab channel prediction\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (L, ab) in enumerate(train_loader):\n",
        "        # Move inputs and targets to GPU if available\n",
        "        L, ab = L.to(device), ab.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: grayscale L channel -> ab color channels\n",
        "        ab_pred = model(L)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(ab_pred, ab)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print statistics\n",
        "        running_loss += loss.item()\n",
        "        if batch_idx % 100 == 99:  # Print every 100 mini-batches\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "# Save the trained model\n",
        "# torch.save(model.state_dict(), 'colorization_model.pth')\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "9cRnbw0dHPsj",
        "outputId": "25a9824a-b84d-4c35-ec8e-4a753d2ef23a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "the input array must have size 3 along `channel_axis`, got (3, 224, 224)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-c42346a2003d>\u001b[0m in \u001b[0;36m<cell line: 74>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mab\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;31m# Move inputs and targets to GPU if available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-c42346a2003d>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Convert the RGB image to Lab color space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mimg_lab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgb2lab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Normalize to [0, 1] before conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Normalize L channel to range [-1, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py\u001b[0m in \u001b[0;36mfixed_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mchannel_axis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;31m# TODO: convert scalars to a tuple in anticipation of eventually\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/skimage/color/colorconv.py\u001b[0m in \u001b[0;36mrgb2lab\u001b[0;34m(rgb, illuminant, observer, channel_axis)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwikipedia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mwiki\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mStandard_illuminant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \"\"\"\n\u001b[0;32m-> 1280\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mxyz2lab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb2xyz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0milluminant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobserver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py\u001b[0m in \u001b[0;36mfixed_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mchannel_axis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;31m# TODO: convert scalars to a tuple in anticipation of eventually\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/skimage/color/colorconv.py\u001b[0m in \u001b[0;36mrgb2xyz\u001b[0;34m(rgb, channel_axis)\u001b[0m\n\u001b[1;32m    851\u001b[0m     \u001b[0;31m# Follow the algorithm from http://www.easyrgb.com/index.php\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m     \u001b[0;31m# except we don't multiply/divide by 100 in the conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_colorarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.04045\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m     \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.055\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1.055\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/skimage/color/colorconv.py\u001b[0m in \u001b[0;36m_prepare_colorarray\u001b[0;34m(arr, force_copy, channel_axis)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;34mf'got {arr.shape}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         )\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mfloat_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_supported_float_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: the input array must have size 3 along `channel_axis`, got (3, 224, 224)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LK7BD19BHXEp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}