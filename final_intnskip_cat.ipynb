{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "55-ktPQMXK5e"
      },
      "outputs": [],
      "source": [
        "from skimage import color\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset class for CIFAR-10\n",
        "class ColorizationDataset(Dataset):\n",
        "    def __init__(self, image_list):\n",
        "        self.image_list = image_list\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),              # Ensure images are in PIL format\n",
        "            transforms.Resize((224, 224)),        # Resize to 224x224\n",
        "            transforms.ToTensor()                 # Convert to Tensor\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.image_list[idx]\n",
        "\n",
        "        # Apply the transformation\n",
        "        img = self.transform(img)\n",
        "\n",
        "        # Convert the resized RGB image to Lab color space\n",
        "        img_lab = color.rgb2lab(np.array(img).transpose(1, 2, 0)).astype(np.float32)\n",
        "\n",
        "        # Normalize the Lab channels\n",
        "        img_lab = (img_lab / 100.0)  # Normalize L, a, b to [-1, 1]\n",
        "        L = img_lab[:, :, 0:1]  # Input: L channel\n",
        "        ab = img_lab[:, :, 1:]  # Target: ab channels\n",
        "\n",
        "        # Convert to PyTorch tensor\n",
        "        L = torch.from_numpy(L).permute(2, 0, 1)  # HxWx1 -> 1xHxW\n",
        "        ab = torch.from_numpy(ab).permute(2, 0, 1)  # HxWx2 -> 2xHxW\n",
        "\n",
        "        return L, ab\n"
      ],
      "metadata": {
        "id": "4kXBXaGQXnK9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load and preprocess CIFAR-10 dataset\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "def load_cifar10_data(batch_size):\n",
        "    # Load CIFAR-10 dataset\n",
        "    train_set = CIFAR10(root='./data', train=True, download=True, transform=None)\n",
        "    test_set = CIFAR10(root='./data', train=False, download=True, transform=None)\n",
        "\n",
        "    # Create custom ColorizationDataset\n",
        "    train_data = ColorizationDataset([np.array(img) for img, _ in train_set])\n",
        "    test_data = ColorizationDataset([np.array(img) for img, _ in test_set])\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader\n"
      ],
      "metadata": {
        "id": "y8d1I9G0XsbC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Fusion block to combine features from ResNet and DenseNet\n",
        "class FusionBlock(nn.Module):\n",
        "    def __init__(self, in_channels_1, in_channels_2):\n",
        "        super(FusionBlock, self).__init__()\n",
        "        # 1x1 convolution to unify the channel size to 256 for both feature maps\n",
        "        self.conv1 = nn.Conv2d(in_channels_1, 256, kernel_size=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels_2, 256, kernel_size=1)\n",
        "        # Another 1x1 convolution to reduce the concatenated output back to 256 channels\n",
        "        self.reduce_channels = nn.Conv2d(512, 256, kernel_size=1)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        # print(f'FusionBlock - Input x1 shape: {x1.shape}, Input x2 shape: {x2.shape}')\n",
        "        x1 = self.conv1(x1)\n",
        "        x2 = self.conv2(x2)\n",
        "        # Concatenate the two feature maps along the channel dimension\n",
        "        x = torch.cat([x1, x2], dim=1)\n",
        "        # print(f'FusionBlock - After concat shape: {x.shape}')\n",
        "        # Reduce the concatenated output back to 256 channels\n",
        "        x = self.reduce_channels(x)\n",
        "        # print(f'FusionBlock - After reducing channels shape: {x.shape}')\n",
        "        return x\n",
        "\n",
        "# Decoder block with upsampling and unified output to 256 channels\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels=256, out_channels=256):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        # Expecting 512 channels from the concatenated feature maps, reducing to 256\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "    def forward(self, x, skip=None):\n",
        "        # print(f'DecoderBlock - Input x shape: {x.shape}')\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        # print(f'DecoderBlock - After conv shape: {x.shape}')\n",
        "        x = self.upsample(x)\n",
        "        # print(f'DecoderBlock - After upsample shape: {x.shape}')\n",
        "\n",
        "        if skip is not None:\n",
        "            # print(f'DecoderBlock - Skip connection shape: {skip.shape}')\n",
        "            # Upsample skip connection if needed to match spatial size\n",
        "            if skip.shape[2:] != x.shape[2:]:\n",
        "                skip = F.interpolate(skip, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
        "                # print(f'DecoderBlock - After skip upsample shape: {skip.shape}')\n",
        "            x = x + skip\n",
        "            # print(f'DecoderBlock - After adding skip shape: {x.shape}')\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "vFA55ZAvZtuh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "from torchvision import models\n",
        "\n"
      ],
      "metadata": {
        "id": "oCEo9EINc8DY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Colorization Model using ResNet50 and DenseNet121\n",
        "class ColorizationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ColorizationModel, self).__init__()\n",
        "\n",
        "        # Pretrained ResNet50 and DenseNet121 as encoders\n",
        "        self.resnet = models.resnet50(pretrained=True)\n",
        "        self.densenet = models.densenet121(pretrained=True)\n",
        "\n",
        "        # Define the layers from which you want to extract features\n",
        "        return_nodes_resnet = {\n",
        "            'layer1': 'resnet_feats_56',   # Feature map size 56x56\n",
        "            'layer2': 'resnet_feats_28',   # Feature map size 28x28\n",
        "            'layer3': 'resnet_feats_14',   # Feature map size 14x14\n",
        "            'layer4': 'resnet_feats_7'     # Feature map size 7x7\n",
        "        }\n",
        "\n",
        "        return_nodes_densenet = {\n",
        "            'features.denseblock1': 'densenet_feats_56',   # Feature map size 56x56\n",
        "            'features.denseblock2': 'densenet_feats_28',   # Feature map size 28x28\n",
        "            'features.denseblock3': 'densenet_feats_14',   # Feature map size 14x14\n",
        "            'features.denseblock4': 'densenet_feats_7'     # Feature map size 7x7\n",
        "        }\n",
        "\n",
        "        # Create feature extractors\n",
        "        self.resnet_extractor = create_feature_extractor(self.resnet, return_nodes=return_nodes_resnet)\n",
        "        self.densenet_extractor = create_feature_extractor(self.densenet, return_nodes=return_nodes_densenet)\n",
        "\n",
        "        # Fusion blocks for multi-level features (each output after concatenation is 512 channels)\n",
        "        self.fusion_56 = FusionBlock(256, 256)  # Concatenate to get 512 channels\n",
        "        self.fusion_28 = FusionBlock(512, 512)  # Concatenate to get 512 channels\n",
        "        self.fusion_14 = FusionBlock(1024, 1024)  # Concatenate to get 512 channels\n",
        "        self.fusion_7 = FusionBlock(2048, 1024)  # Concatenate to get 512 channels\n",
        "\n",
        "        # Decoder blocks with upsampling\n",
        "        self.decoder_7 = DecoderBlock(256)   # Input 512 from fusion_7\n",
        "        self.decoder_14 = DecoderBlock(256)  # Input 512 from fusion_14\n",
        "        self.decoder_28 = DecoderBlock(256)  # Input 512 from fusion_28\n",
        "        self.decoder_56 = DecoderBlock(256)  # Input 512 from fusion_56\n",
        "\n",
        "        # Final output layer (predict ab channels)\n",
        "        self.final_conv = nn.Conv2d(256, 2, kernel_size=3, padding=1)\n",
        "        self.upsample_final = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Replicate grayscale input to 3 channels for ResNet and DenseNet\n",
        "        x_rgb = x.repeat(1, 3, 1, 1)\n",
        "        # print(f'Forward Pass - Initial input shape: {x_rgb.shape}')\n",
        "\n",
        "        # Extract features from different stages of ResNet and DenseNet\n",
        "        resnet_feats = self.resnet_extractor(x_rgb)\n",
        "        densenet_feats = self.densenet_extractor(x_rgb)\n",
        "\n",
        "        # Get features for each level\n",
        "        resnet_feats_56 = resnet_feats['resnet_feats_56']\n",
        "        resnet_feats_28 = resnet_feats['resnet_feats_28']\n",
        "        resnet_feats_14 = resnet_feats['resnet_feats_14']\n",
        "        resnet_feats_7 = resnet_feats['resnet_feats_7']\n",
        "\n",
        "        densenet_feats_56 = densenet_feats['densenet_feats_56']\n",
        "        densenet_feats_28 = densenet_feats['densenet_feats_28']\n",
        "        densenet_feats_14 = densenet_feats['densenet_feats_14']\n",
        "        densenet_feats_7 = densenet_feats['densenet_feats_7']\n",
        "\n",
        "        # Fusion of multi-level features\n",
        "        fusion_56 = self.fusion_56(resnet_feats_56, densenet_feats_56)\n",
        "        fusion_28 = self.fusion_28(resnet_feats_28, densenet_feats_28)\n",
        "        fusion_14 = self.fusion_14(resnet_feats_14, densenet_feats_14)\n",
        "        fusion_7 = self.fusion_7(resnet_feats_7, densenet_feats_7)\n",
        "\n",
        "        # Decoder with skip connections and unified channels\n",
        "        decoded_7 = self.decoder_7(fusion_7)            # 7x7 -> 14x14\n",
        "        decoded_14 = self.decoder_14(decoded_7, fusion_14)  # 14x14 -> 28x28\n",
        "        decoded_28 = self.decoder_28(decoded_14, fusion_28)  # 28x28 -> 56x56\n",
        "        decoded_56 = self.decoder_56(decoded_28, fusion_56)  # 56x56 -> Final output\n",
        "\n",
        "        # Final prediction for ab channels\n",
        "        ab_pred = self.final_conv(decoded_56)\n",
        "        ab_pred = self.upsample_final(ab_pred)\n",
        "        # print(f'Forward Pass - Final output shape: {ab_pred.shape}')\n",
        "\n",
        "        return ab_pred\n"
      ],
      "metadata": {
        "id": "u2b9P5n5Xyg-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "WOoxsT-zX6P6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, test_loader, num_epochs=2, lr=0.001):\n",
        "    # Define optimizer and loss function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()  # Assuming you're using MSE loss for the ab channels\n",
        "\n",
        "    model.train()  # Set model to training mode\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, (L, ab) in enumerate(tqdm(train_loader)):\n",
        "            # Move data to the appropriate device (GPU or CPU)\n",
        "            L = L.to(device)\n",
        "            ab = ab.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            ab_pred = model(L)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(ab_pred, ab)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate loss for reporting\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Print the loss every 500 batches\n",
        "            if (i + 1) % 500 == 0:\n",
        "                avg_loss = running_loss / 500\n",
        "                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {avg_loss:.4f}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "        # Validation at the end of each epoch\n",
        "        validate_model(model, test_loader)\n",
        "\n",
        "def validate_model(model, test_loader):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    running_loss = 0.0\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for L, ab in test_loader:\n",
        "            L = L.to(device)\n",
        "            ab = ab.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            ab_pred = model(L)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(ab_pred, ab)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(test_loader)\n",
        "    print(f'Validation Loss: {avg_loss:.4f}')\n",
        "    model.train()  # Set model back to training mode after validation\n"
      ],
      "metadata": {
        "id": "WkmeJeGU0e2K"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load CIFAR-10 data\n",
        "    batch_size = 8  # You can adjust this based on your system\n",
        "    train_loader, test_loader = load_cifar10_data(batch_size)\n",
        "\n",
        "    # Initialize the model\n",
        "    model = ColorizationModel().to(device)\n",
        "\n",
        "    # Train the model\n",
        "    train_model(model, train_loader, test_loader, num_epochs=2, lr=0.001)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QysW-SUYAfM",
        "outputId": "4d877b26-346b-4d17-bcbb-ff7dbe0ee80f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "  8%|▊         | 500/6250 [02:41<29:43,  3.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2], Step [500/6250], Loss: 0.0289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▌        | 1000/6250 [05:20<28:00,  3.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2], Step [1000/6250], Loss: 0.0171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 24%|██▍       | 1500/6250 [08:07<24:24,  3.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2], Step [1500/6250], Loss: 0.0171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 32%|███▏      | 2000/6250 [10:45<21:51,  3.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2], Step [2000/6250], Loss: 0.0185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 2500/6250 [13:21<19:09,  3.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2], Step [2500/6250], Loss: 0.0163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 48%|████▊     | 3000/6250 [15:58<16:33,  3.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2], Step [3000/6250], Loss: 0.0164\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 56%|█████▌    | 3500/6250 [18:35<14:03,  3.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2], Step [3500/6250], Loss: 0.0154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 64%|██████▍   | 4000/6250 [21:12<12:20,  3.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2], Step [4000/6250], Loss: 0.0155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 72%|███████▏  | 4500/6250 [23:48<08:51,  3.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2], Step [4500/6250], Loss: 0.0158\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 5000/6250 [26:24<06:31,  3.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2], Step [5000/6250], Loss: 0.0158\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 88%|████████▊ | 5500/6250 [29:03<04:13,  2.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2], Step [5500/6250], Loss: 0.0157\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|█████████▌| 6000/6250 [31:40<01:15,  3.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2], Step [6000/6250], Loss: 0.0157\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6250/6250 [32:59<00:00,  3.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 500/6250 [02:36<33:01,  2.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/2], Step [500/6250], Loss: 0.0154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▌        | 1000/6250 [05:12<26:34,  3.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/2], Step [1000/6250], Loss: 0.0149\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 24%|██▍       | 1500/6250 [07:48<24:15,  3.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/2], Step [1500/6250], Loss: 0.0148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 32%|███▏      | 2000/6250 [10:25<23:06,  3.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/2], Step [2000/6250], Loss: 0.0156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 2500/6250 [13:01<18:57,  3.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/2], Step [2500/6250], Loss: 0.0160\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 48%|████▊     | 3000/6250 [15:37<17:07,  3.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/2], Step [3000/6250], Loss: 0.0153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 56%|█████▌    | 3500/6250 [18:13<13:54,  3.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/2], Step [3500/6250], Loss: 0.0159\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 64%|██████▍   | 4000/6250 [20:49<11:29,  3.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/2], Step [4000/6250], Loss: 0.0151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 72%|███████▏  | 4500/6250 [23:25<09:14,  3.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/2], Step [4500/6250], Loss: 0.0153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 5000/6250 [26:01<06:24,  3.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/2], Step [5000/6250], Loss: 0.0159\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 88%|████████▊ | 5500/6250 [28:37<03:59,  3.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/2], Step [5500/6250], Loss: 0.0156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|█████████▌| 6000/6250 [31:13<01:17,  3.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/2], Step [6000/6250], Loss: 0.0156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6250/6250 [32:31<00:00,  3.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'final_intnskip_cat.pth')"
      ],
      "metadata": {
        "id": "a4UucE9g2W1P"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0a01bqVuYDRE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}