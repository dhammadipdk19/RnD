{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPt6wrpM7q/tJhPgLnJjD3I"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"lRATlajEwt6L"},"outputs":[],"source":["## Importing required Libraries\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms, models\n","import numpy as np\n","from skimage.color import rgb2lab"]},{"cell_type":"code","source":["# To GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"4fvXKzkWw_7Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to convert RGB images to L, a, and b channels in Lab color space\n","def rgb_to_lab(images):\n","    l_channels = []\n","    ab_channels = []\n","    for img in images:\n","        img = img.permute(1, 2, 0).cpu().numpy()  # Convert to HWC format and move to CPU\n","        lab_image = rgb2lab(img)  # Convert to CIE-Lab\n","\n","        # Normalize L, a, and b channels\n","        L_channel = lab_image[:, :, 0] / 100.0  # Normalize L channel to [0, 1]\n","        a_channel = (lab_image[:, :, 1] + 128) / 255.0  # Normalize a channel to [0, 1]\n","        b_channel = (lab_image[:, :, 2] + 128) / 255.0  # Normalize b channel to [0, 1]\n","\n","        l_channels.append(L_channel)\n","        ab_channels.append(np.stack((a_channel, b_channel), axis=-1))  # Stack a and b\n","\n","    # Convert to PyTorch tensors\n","    L = torch.tensor(np.stack(l_channels), dtype=torch.float32).unsqueeze(1).to(device)  # (N, 1, H, W)\n","    ab = torch.tensor(np.stack(ab_channels), dtype=torch.float32).permute(0, 3, 1, 2).to(device)  # (N, 2, H, W)\n","    return L, ab"],"metadata":{"id":"knDYdjKRxBlK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# CIFAR10 Dataset Loader\n","def load_cifar10_dataset(batch_size=8, num_workers=2):\n","    transform = transforms.Compose([\n","        transforms.Resize((224, 224)),  # Resize to match ResNet/DenseNet input size\n","        transforms.ToTensor(),  # Convert to tensor and scale to [0, 1]\n","    ])\n","\n","    # Load the CIFAR-10 dataset\n","    train_set = datasets.CIFAR10(root='./data/cifar10', train=True, download=True, transform=transform)\n","    test_set = datasets.CIFAR10(root='./data/cifar10', train=False, download=True, transform=transform)\n","\n","    # Create DataLoaders for train and test sets\n","    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n","    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n","\n","    return train_loader, test_loader"],"metadata":{"id":"-1I2LhwSxDED"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Model Architecture\n","\n","class ColorizationModel(nn.Module):\n","    def __init__(self):\n","        super(ColorizationModel, self).__init__()\n","\n","        # Pre-trained ResNet50 encoder Model (modified to accept 1 channel input i.e., L-Channal)\n","        self.encoder_resnet = models.resnet50(weights='IMAGENET1K_V1')\n","        self.encoder_resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.encoder_resnet = nn.Sequential(*list(self.encoder_resnet.children())[:-2])  # Removing not required layers\n","\n","        # Pre-trained DenseNet121 encoder (modified to accept 1 channel input i.e., L-Channal)\n","        self.encoder_densenet = models.densenet121(weights='IMAGENET1K_V1')\n","        self.encoder_densenet.features.conv0 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.encoder_densenet = nn.Sequential(*list(self.encoder_densenet.children())[:-1])  # Removing not required layers\n","\n","        # Pooling layer to downsample DenseNet output to 7x7\n","        self.downsample_densenet = nn.AdaptiveAvgPool2d((7, 7))\n","\n","        # Fusion Blocks\n","        self.fusion_block1 = nn.Sequential(\n","            nn.Conv2d(2048 + 1024, 256, kernel_size=1),   # concatnated inchannals to outchannals\n","            nn.BatchNorm2d(256),\n","            nn.ReLU()\n","        )\n","\n","        self.fusion_block2 = nn.Sequential(\n","            nn.Conv2d(256 + 256, 256, kernel_size=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU()\n","        )\n","\n","        self.fusion_block3 = nn.Sequential(\n","            nn.Conv2d(256 + 256, 256, kernel_size=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU()\n","        )\n","\n","        self.fusion_block4 = nn.Sequential(\n","            nn.Conv2d(256 + 256, 256, kernel_size=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU()\n","        )\n","\n","        # Decoder Blocks\n","        self.decoder_block1 = nn.Sequential(\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.Upsample(scale_factor=2)\n","        )\n","\n","        self.decoder_block2 = nn.Sequential(\n","            nn.Conv2d(512, 256, kernel_size=3, padding=1),  # Input Channals -> 512 (Concatnated db1 output and fb3 output)\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.Upsample(scale_factor=2)\n","        )\n","\n","        self.decoder_block3 = nn.Sequential(\n","            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.Upsample(scale_factor=2)\n","        )\n","\n","        self.decoder_block4 = nn.Sequential(\n","            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.Upsample(scale_factor=2)\n","        )\n","\n","        self.decoder_block5 = nn.Sequential(\n","            nn.Conv2d(256, 2, kernel_size=3, padding=1),\n","            nn.Tanh(),  # Output Range [-1, 1]\n","            nn.Upsample(scale_factor=2)  # Upsample to 224 x 224 (Original Spacial resolution)\n","        )\n","\n","        # Upsampling layers for skip connections -> to match input concatenation to decoder blocks\n","        self.upsample_fb3 = nn.Upsample(scale_factor=2)  # Upsample from 7x7 to 14x14\n","        self.upsample_fb2 = nn.Upsample(scale_factor=4)  # Upsample from 7x7 to 28x28\n","        self.upsample_fb1 = nn.Upsample(scale_factor=8)  # Upsample from 7x7 to 56x56\n","\n","    def forward(self, x):\n","        # Encoder\n","        x_resnet = self.encoder_resnet(x)  # ResNet output\n","        x_densenet = self.encoder_densenet(x)  # DenseNet output\n","        x_densenet = self.downsample_densenet(x_densenet)  # Downsample DenseNet output\n","\n","        # Fusion Blocks\n","        fb1_input = torch.cat([x_resnet, x_densenet], dim=1)  # 2048 + 1024\n","        fb1_output = self.fusion_block1(fb1_input)\n","\n","        fb2_input = torch.cat([fb1_output, fb1_output], dim=1)  # Use previous output only\n","        fb2_output = self.fusion_block2(fb2_input)\n","\n","        fb3_input = torch.cat([fb2_output, fb2_output], dim=1)  # Use previous output only\n","        fb3_output = self.fusion_block3(fb3_input)\n","\n","        fb4_input = torch.cat([fb3_output, fb3_output], dim=1)  # Use previous output only\n","        fb4_output = self.fusion_block4(fb4_input)\n","\n","        # Decoder with Skip Connections\n","        db1_output = self.decoder_block1(fb4_output)\n","\n","        # Upsample fb3 to match db1_output's size\n","        fb3_output_upsampled = self.upsample_fb3(fb3_output)\n","        db2_input = torch.cat([db1_output, fb3_output_upsampled], dim=1)  # Concatenate db1_output with fb3 (upsampled)\n","        db2_output = self.decoder_block2(db2_input)\n","\n","        # Upsample fb2 to match db2_output's size\n","        fb2_output_upsampled = self.upsample_fb2(fb2_output)\n","        db3_input = torch.cat([db2_output, fb2_output_upsampled], dim=1)  # Concatenate db2_output with fb2 (upsampled)\n","        db3_output = self.decoder_block3(db3_input)\n","\n","        # Upsample fb1 to match db3_output's size\n","        fb1_output_upsampled = self.upsample_fb1(fb1_output)\n","        db4_input = torch.cat([db3_output, fb1_output_upsampled], dim=1)  # Concatenate db3_output with fb1 (upsampled)\n","        db4_output = self.decoder_block4(db4_input)\n","\n","        output = self.decoder_block5(db4_output)\n","\n","        return output\n"],"metadata":{"id":"cb6blAAWxEkZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","# Training loop\n","def train(model, train_loader, criterion, optimizer, num_epochs):\n","    model.train()  # Set the model to training mode\n","    for epoch in range(num_epochs):\n","        for batch_idx, batch in enumerate(train_loader):\n","            images = batch[0].to(device)  # Move images to device\n","\n","            # Convert RGB to L and ab channels\n","            L, ab_target = rgb_to_lab(images)\n","\n","            # Zero gradients\n","            optimizer.zero_grad()\n","\n","            # Forward pass: model predicts 'a' and 'b' channels from 'L' channel\n","            ab_pred = model(L)  # The input is now just the L channel\n","\n","            # Compute loss between predicted ab channels and ground truth ab channels\n","            loss = criterion(ab_pred, ab_target)\n","            loss.backward()  # Backward pass\n","            optimizer.step()  # Update weights\n","\n","            # Print loss every 500 batches\n","            if batch_idx % 500 == 0:\n","                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}')"],"metadata":{"id":"uVIArhhnyvX1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example usage\n","if __name__ == \"__main__\":\n","    # Load the CIFAR10 dataset\n","    batch_size = 8\n","    train_loader, test_loader = load_cifar10_dataset(batch_size=batch_size)\n","\n","    # Initialize the model\n","    model = ColorizationModel().to(device)  # Move model to GPU if available\n","\n","    # Define loss function and optimizer\n","    criterion = nn.MSELoss()  # Loss for comparing predicted ab with ground truth ab\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","    # Train the model\n","    num_epochs = 2\n","    train(model, train_loader, criterion, optimizer, num_epochs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":497},"id":"k2QYCCWHy0eW","executionInfo":{"status":"error","timestamp":1729368170135,"user_tz":-330,"elapsed":20378,"user":{"displayName":"DHAMMADIP KAMBLE","userId":"08382130647714499133"}},"outputId":"7154d250-344c-4cb6-d15f-9b6f30589241"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar10/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:05<00:00, 29593531.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar10/cifar-10-python.tar.gz to ./data/cifar10\n","Files already downloaded and verified\n"]},{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","100%|██████████| 97.8M/97.8M [00:00<00:00, 167MB/s]\n","Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n","100%|██████████| 30.8M/30.8M [00:00<00:00, 135MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/2], Batch [0/6250], Loss: 0.3705\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-613ee4e873ed>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-6-b48ce29b0dce>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;31m# Convert RGB to L and ab channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mab_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrgb_to_lab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m# Zero gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-bc5122e8aad6>\u001b[0m in \u001b[0;36mrgb_to_lab\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert to HWC format and move to CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mlab_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrgb2lab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert to CIE-Lab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Normalize L, a, and b channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py\u001b[0m in \u001b[0;36mfixed_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mchannel_axis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;31m# TODO: convert scalars to a tuple in anticipation of eventually\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/skimage/color/colorconv.py\u001b[0m in \u001b[0;36mrgb2lab\u001b[0;34m(rgb, illuminant, observer, channel_axis)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwikipedia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mwiki\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mStandard_illuminant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \"\"\"\n\u001b[0;32m-> 1280\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mxyz2lab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb2xyz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0milluminant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobserver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/skimage/_shared/utils.py\u001b[0m in \u001b[0;36mfixed_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mchannel_axis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;31m# TODO: convert scalars to a tuple in anticipation of eventually\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/skimage/color/colorconv.py\u001b[0m in \u001b[0;36mrgb2xyz\u001b[0;34m(rgb, channel_axis)\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_colorarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.04045\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m     \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.055\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1.055\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;36m12.92\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mxyz_from_rgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["import torch\n","\n","# After training is complete\n","model_save_path = \"colorization_model_cat.pth\"\n","\n","# Save the model's state_dict (recommended way to save models in PyTorch)\n","torch.save(model.state_dict(), model_save_path)\n","\n","print(f\"Model saved to {model_save_path}\")"],"metadata":{"id":"vJR6UNz_y5CN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"AcQw3WySz-fk"},"execution_count":null,"outputs":[]}]}