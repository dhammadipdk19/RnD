{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPNV9jVkGWql"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TBwysBz_Gg6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from torch.utils.data import DataLoader\n",
        "# from torchvision import datasets, transforms, models\n",
        "\n",
        "# # Function to scale images to [0, 255]\n",
        "# def scale_to_255(x):\n",
        "#     return x * 255\n",
        "\n",
        "# # CIFAR10 Dataset Loader\n",
        "# def load_cifar10_dataset(batch_size=8, num_workers=2):\n",
        "#     transform = transforms.Compose([\n",
        "#         transforms.Resize((224, 224)),  # Resize to match ResNet/DenseNet input size\n",
        "#         transforms.ToTensor(),  # Convert to tensor and scale to [0, 1]\n",
        "#         transforms.Lambda(scale_to_255),  # Scale to [0, 255]\n",
        "#     ])\n",
        "\n",
        "#     # Load the CIFAR-10 dataset\n",
        "#     train_set = datasets.CIFAR10(root='./data/cifar10', train=True, download=True, transform=transform)\n",
        "#     test_set = datasets.CIFAR10(root='./data/cifar10', train=False, download=True, transform=transform)\n",
        "\n",
        "#     # Create DataLoaders for train and test sets\n",
        "#     train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "#     test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "#     return train_loader, test_loader\n",
        "\n",
        "# # Define the colorization model\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torchvision.models as models\n",
        "\n",
        "# class ColorizationModel(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(ColorizationModel, self).__init__()\n",
        "\n",
        "#         # Pre-trained ResNet50 encoder\n",
        "#         self.encoder_resnet = nn.Sequential(\n",
        "#             *list(models.resnet50(weights='IMAGENET1K_V1').children())[:-2]\n",
        "#         )\n",
        "\n",
        "#         # Pre-trained DenseNet121 encoder\n",
        "#         self.encoder_densenet = nn.Sequential(\n",
        "#             *list(models.densenet121(weights='IMAGENET1K_V1').children())[:-1]  # Use all layers except the classifier\n",
        "#         )\n",
        "\n",
        "#         # Pooling layer to downsample DenseNet output to 7x7\n",
        "#         self.downsample_densenet = nn.AdaptiveAvgPool2d((7, 7))\n",
        "\n",
        "#         # Fusion Blocks\n",
        "#         self.fusion_block1 = nn.Sequential(\n",
        "#             nn.Conv2d(2048 + 1024, 256, kernel_size=1),  # Adjust input channels\n",
        "#             nn.BatchNorm2d(256),\n",
        "#             nn.ReLU()\n",
        "#         )\n",
        "\n",
        "#         self.fusion_block2 = nn.Sequential(\n",
        "#             nn.Conv2d(256 + 256, 256, kernel_size=1),  # Adjust input channels\n",
        "#             nn.BatchNorm2d(256),\n",
        "#             nn.ReLU()\n",
        "#         )\n",
        "\n",
        "#         self.fusion_block3 = nn.Sequential(\n",
        "#             nn.Conv2d(256 + 256, 256, kernel_size=1),  # Adjust input channels\n",
        "#             nn.BatchNorm2d(256),\n",
        "#             nn.ReLU()\n",
        "#         )\n",
        "\n",
        "#         self.fusion_block4 = nn.Sequential(\n",
        "#             nn.Conv2d(256 + 256, 256, kernel_size=1),  # Adjust input channels\n",
        "#             nn.BatchNorm2d(256),\n",
        "#             nn.ReLU()\n",
        "#         )\n",
        "\n",
        "#         # Decoder Blocks\n",
        "#         self.decoder_block1 = nn.Sequential(\n",
        "#             nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "#             nn.BatchNorm2d(256),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Upsample(scale_factor=2)\n",
        "#         )\n",
        "\n",
        "#         self.decoder_block2 = nn.Sequential(\n",
        "#             nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "#             nn.BatchNorm2d(256),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Upsample(scale_factor=2)\n",
        "#         )\n",
        "\n",
        "#         self.decoder_block3 = nn.Sequential(\n",
        "#             nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "#             nn.BatchNorm2d(256),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Upsample(scale_factor=2)\n",
        "#         )\n",
        "\n",
        "#         self.decoder_block4 = nn.Sequential(\n",
        "#             nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "#             nn.BatchNorm2d(256),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Upsample(scale_factor=2)\n",
        "#         )\n",
        "\n",
        "#         self.decoder_block5 = nn.Sequential(\n",
        "#             nn.Conv2d(256, 2, kernel_size=3, padding=1),\n",
        "#             nn.Tanh()  # Use Tanh to match output range [-1, 1]\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Encoder\n",
        "#         x_resnet = self.encoder_resnet(x)  # ResNet output\n",
        "#         x_densenet = self.encoder_densenet(x)  # DenseNet output\n",
        "#         x_densenet = self.downsample_densenet(x_densenet)  # Downsample DenseNet output\n",
        "\n",
        "#         print(f\"ResNet output shape: {x_resnet.shape}\")  # Should be [8, 2048, 7, 7]\n",
        "#         print(f\"DenseNet output shape after downsampling: {x_densenet.shape}\")  # Should be [8, 1024, 7, 7]\n",
        "\n",
        "#         # Fusion Blocks\n",
        "#         fb1_input = torch.cat([x_resnet, x_densenet], dim=1)  # 2048 + 1024\n",
        "#         fb1_output = self.fusion_block1(fb1_input)\n",
        "#         print(f\"Fusion Block 1 output shape: {fb1_output.shape}\")  # Should be [8, 256, 7, 7]\n",
        "\n",
        "#         # For the next fusion block, adjust as necessary based on your architecture\n",
        "#         fb2_input = torch.cat([fb1_output, fb1_output], dim=1)  # Use previous output only\n",
        "#         fb2_output = self.fusion_block2(fb2_input)\n",
        "#         print(f\"Fusion Block 2 output shape: {fb2_output.shape}\")  # Should be [8, 256, 7, 7]\n",
        "\n",
        "#         fb3_input = torch.cat([fb2_output, fb2_output], dim=1)  # Use previous output only\n",
        "#         fb3_output = self.fusion_block3(fb3_input)\n",
        "#         print(f\"Fusion Block 3 output shape: {fb3_output.shape}\")  # Should be [8, 256, 7, 7]\n",
        "\n",
        "#         fb4_input = torch.cat([fb3_output, fb3_output], dim=1)  # Use previous output only\n",
        "#         fb4_output = self.fusion_block4(fb4_input)\n",
        "#         print(f\"Fusion Block 4 output shape: {fb4_output.shape}\")  # Should be [8, 256, 7, 7]\n",
        "\n",
        "#         # Decoder\n",
        "#         db1_output = self.decoder_block1(fb4_output)\n",
        "#         print(f\"Decoder Block 1 output shape: {db1_output.shape}\")\n",
        "\n",
        "#         db2_output = self.decoder_block2(db1_output)\n",
        "#         print(f\"Decoder Block 2 output shape: {db2_output.shape}\")\n",
        "\n",
        "#         db3_output = self.decoder_block3(db2_output)\n",
        "#         print(f\"Decoder Block 3 output shape: {db3_output.shape}\")\n",
        "\n",
        "#         db4_output = self.decoder_block4(db3_output)\n",
        "#         print(f\"Decoder Block 4 output shape: {db4_output.shape}\")\n",
        "\n",
        "#         output = self.decoder_block5(db4_output)\n",
        "#         print(f\"Final output shape: {output.shape}\")\n",
        "\n",
        "#         return output\n",
        "\n",
        "# # Example usage\n",
        "# if __name__ == \"__main__\":\n",
        "#     model = ColorizationModel()\n",
        "#     model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "#     # Example input\n",
        "#     images = torch.randn(8, 3, 224, 224)  # Batch of 8 images\n",
        "#     outputs = model(images)  # Forward pass\n",
        "#     print(\"Output shape:\", outputs.size())  # Check output shape\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Example usage\n",
        "# batch_size = 8\n",
        "# train_loader, test_loader = load_cifar10_dataset(batch_size=batch_size)\n",
        "\n",
        "# # Initialize the model and print the architecture\n",
        "# model = ColorizationModel()\n",
        "# # print(model)\n",
        "\n",
        "# # Check the first batch\n",
        "# for images, labels in train_loader:\n",
        "#     images = images.view(-1, 3, 224, 224)  # Reshape to [B, 3, H, W] for input\n",
        "#     print(\"Input shape:\", images.size())\n",
        "\n",
        "#     outputs = model(images)  # Forward pass through the model\n",
        "#     print(\"Output shape:\", outputs.size())\n",
        "#     break  # Break after the first batch\n"
      ],
      "metadata": {
        "id": "Drpd_NsCJRfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "from skimage.color import rgb2lab"
      ],
      "metadata": {
        "id": "QGPWnnEGLCym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "fhWTxkqORDFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from skimage.color import rgb2lab\n",
        "\n",
        "# Function to scale images to [0, 255]\n",
        "def scale_to_255(x):\n",
        "    return x * 255\n",
        "\n",
        "def rgb_to_ab(images):\n",
        "    a_channels = []\n",
        "    b_channels = []\n",
        "    for img in images:  # Iterate through each image in the batch\n",
        "        img = img.permute(1, 2, 0).cpu().numpy()  # Change to HWC format and move to CPU\n",
        "        lab_image = rgb2lab(img)  # Convert to CIE-Lab\n",
        "\n",
        "        # Normalize L, a, and b channels\n",
        "        L_channel = lab_image[:, :, 0] / 100.0  # Normalize L channel\n",
        "        a_channel = (lab_image[:, :, 1] + 128) / 255.0  # Normalize a channel\n",
        "        b_channel = (lab_image[:, :, 2] + 128) / 255.0  # Normalize b channel\n",
        "\n",
        "        # Collect normalized channels\n",
        "        a_channels.append(a_channel)\n",
        "        b_channels.append(b_channel)\n",
        "\n",
        "    # Stack the a and b channels and convert to tensors\n",
        "    return (\n",
        "        torch.tensor(np.stack(a_channels), dtype=torch.float32).to(device),  # Move to device\n",
        "        torch.tensor(np.stack(b_channels), dtype=torch.float32).to(device)   # Move to device\n",
        "    )\n"
      ],
      "metadata": {
        "id": "L92jHuYVMpPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR10 Dataset Loader\n",
        "def load_cifar10_dataset(batch_size=8, num_workers=2):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # Resize to match ResNet/DenseNet input size\n",
        "        transforms.ToTensor(),  # Convert to tensor and scale to [0, 1]\n",
        "        transforms.Lambda(scale_to_255),  # Scale to [0, 255]\n",
        "    ])\n",
        "\n",
        "    # Load the CIFAR-10 dataset\n",
        "    train_set = datasets.CIFAR10(root='./data/cifar10', train=True, download=True, transform=transform)\n",
        "    test_set = datasets.CIFAR10(root='./data/cifar10', train=False, download=True, transform=transform)\n",
        "\n",
        "    # Create DataLoaders for train and test sets\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "5vdjyaTQPMDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the colorization model (same as before)\n",
        "class ColorizationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ColorizationModel, self).__init__()\n",
        "\n",
        "        # Pre-trained ResNet50 encoder\n",
        "        self.encoder_resnet = nn.Sequential(\n",
        "            *list(models.resnet50(weights='IMAGENET1K_V1').children())[:-2]\n",
        "        )\n",
        "\n",
        "        # Pre-trained DenseNet121 encoder\n",
        "        self.encoder_densenet = nn.Sequential(\n",
        "            *list(models.densenet121(weights='IMAGENET1K_V1').children())[:-1]  # Use all layers except the classifier\n",
        "        )\n",
        "\n",
        "        # Pooling layer to downsample DenseNet output to 7x7\n",
        "        self.downsample_densenet = nn.AdaptiveAvgPool2d((7, 7))\n",
        "\n",
        "        # Fusion Blocks\n",
        "        self.fusion_block1 = nn.Sequential(\n",
        "            nn.Conv2d(2048 + 1024, 256, kernel_size=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fusion_block2 = nn.Sequential(\n",
        "            nn.Conv2d(256 + 256, 256, kernel_size=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fusion_block3 = nn.Sequential(\n",
        "            nn.Conv2d(256 + 256, 256, kernel_size=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fusion_block4 = nn.Sequential(\n",
        "            nn.Conv2d(256 + 256, 256, kernel_size=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder Blocks\n",
        "        self.decoder_block1 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2)\n",
        "        )\n",
        "\n",
        "        self.decoder_block2 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2)\n",
        "        )\n",
        "\n",
        "        self.decoder_block3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2)\n",
        "        )\n",
        "\n",
        "        self.decoder_block4 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2)\n",
        "        )\n",
        "\n",
        "        self.decoder_block5 = nn.Sequential(\n",
        "            nn.Conv2d(256, 2, kernel_size=3, padding=1),\n",
        "            nn.Tanh()  # Use Tanh to match output range [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x_resnet = self.encoder_resnet(x)  # ResNet output\n",
        "        x_densenet = self.encoder_densenet(x)  # DenseNet output\n",
        "        x_densenet = self.downsample_densenet(x_densenet)  # Downsample DenseNet output\n",
        "\n",
        "        # Fusion Blocks\n",
        "        fb1_input = torch.cat([x_resnet, x_densenet], dim=1)  # 2048 + 1024\n",
        "        fb1_output = self.fusion_block1(fb1_input)\n",
        "\n",
        "        fb2_input = torch.cat([fb1_output, fb1_output], dim=1)  # Use previous output only\n",
        "        fb2_output = self.fusion_block2(fb2_input)\n",
        "\n",
        "        fb3_input = torch.cat([fb2_output, fb2_output], dim=1)  # Use previous output only\n",
        "        fb3_output = self.fusion_block3(fb3_input)\n",
        "\n",
        "        fb4_input = torch.cat([fb3_output, fb3_output], dim=1)  # Use previous output only\n",
        "        fb4_output = self.fusion_block4(fb4_input)\n",
        "\n",
        "        # Decoder\n",
        "        db1_output = self.decoder_block1(fb4_output)\n",
        "        db2_output = self.decoder_block2(db1_output)\n",
        "        db3_output = self.decoder_block3(db2_output)\n",
        "        db4_output = self.decoder_block4(db3_output)\n",
        "\n",
        "        output = self.decoder_block5(db4_output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "ldarWvmiPO2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_idx, batch in enumerate(train_loader):  # Use enumerate to track batch index\n",
        "            images = batch[0]  # Assuming the first element is the images\n",
        "            images = images / 255.0  # Normalize images to [0, 1]\n",
        "            images = images.to(device)  # Move images to the correct device\n",
        "\n",
        "            # Convert images to 'a' and 'b' channels\n",
        "            a_channel, b_channel = rgb_to_ab(images)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)  # Forward pass\n",
        "\n",
        "            # Combine a and b channels\n",
        "            target = torch.stack((a_channel, b_channel), dim=1)  # Combine a and b channels\n",
        "\n",
        "            # Resize target to match model's output size\n",
        "            target_resized = F.interpolate(target, size=(112, 112), mode='bilinear', align_corners=False)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, target_resized)  # Ensure shapes match\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Update weights\n",
        "\n",
        "            # Print loss every 10 batches\n",
        "            if batch_idx % 500 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}')  # Print loss for monitoring\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vV9X79ftPTSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Load the CIFAR10 dataset\n",
        "    batch_size = 8\n",
        "    train_loader, test_loader = load_cifar10_dataset(batch_size=batch_size)\n",
        "\n",
        "    # Initialize the model\n",
        "    model = ColorizationModel().to(device)  # Move model to GPU if available\n",
        "\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Train the model\n",
        "    train(model, train_loader, criterion, optimizer, num_epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoV5fuSePahq",
        "outputId": "b8dd3be5-00d3-4957-f646-bdc0d1649ac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/5], Batch [0/6250], Loss: 0.1977\n",
            "Epoch [1/5], Batch [500/6250], Loss: 0.0061\n",
            "Epoch [1/5], Batch [1000/6250], Loss: 0.0053\n",
            "Epoch [1/5], Batch [1500/6250], Loss: 0.0025\n",
            "Epoch [1/5], Batch [2000/6250], Loss: 0.0023\n",
            "Epoch [1/5], Batch [2500/6250], Loss: 0.0016\n",
            "Epoch [1/5], Batch [3000/6250], Loss: 0.0014\n",
            "Epoch [1/5], Batch [3500/6250], Loss: 0.0004\n",
            "Epoch [1/5], Batch [4000/6250], Loss: 0.0006\n",
            "Epoch [1/5], Batch [4500/6250], Loss: 0.0010\n",
            "Epoch [1/5], Batch [5000/6250], Loss: 0.0008\n",
            "Epoch [1/5], Batch [5500/6250], Loss: 0.0007\n",
            "Epoch [1/5], Batch [6000/6250], Loss: 0.0004\n",
            "Epoch [2/5], Batch [0/6250], Loss: 0.0017\n",
            "Epoch [2/5], Batch [500/6250], Loss: 0.0004\n",
            "Epoch [2/5], Batch [1000/6250], Loss: 0.0010\n",
            "Epoch [2/5], Batch [1500/6250], Loss: 0.0009\n",
            "Epoch [2/5], Batch [2000/6250], Loss: 0.0005\n",
            "Epoch [2/5], Batch [2500/6250], Loss: 0.0005\n",
            "Epoch [2/5], Batch [3000/6250], Loss: 0.0010\n",
            "Epoch [2/5], Batch [3500/6250], Loss: 0.0009\n",
            "Epoch [2/5], Batch [4000/6250], Loss: 0.0003\n",
            "Epoch [2/5], Batch [4500/6250], Loss: 0.0005\n",
            "Epoch [2/5], Batch [5000/6250], Loss: 0.0005\n",
            "Epoch [2/5], Batch [5500/6250], Loss: 0.0006\n",
            "Epoch [2/5], Batch [6000/6250], Loss: 0.0006\n",
            "Epoch [3/5], Batch [0/6250], Loss: 0.0007\n",
            "Epoch [3/5], Batch [500/6250], Loss: 0.0004\n",
            "Epoch [3/5], Batch [1000/6250], Loss: 0.0004\n",
            "Epoch [3/5], Batch [1500/6250], Loss: 0.0009\n",
            "Epoch [3/5], Batch [2000/6250], Loss: 0.0003\n",
            "Epoch [3/5], Batch [2500/6250], Loss: 0.0009\n",
            "Epoch [3/5], Batch [3000/6250], Loss: 0.0007\n",
            "Epoch [3/5], Batch [3500/6250], Loss: 0.0002\n",
            "Epoch [3/5], Batch [4000/6250], Loss: 0.0002\n",
            "Epoch [3/5], Batch [4500/6250], Loss: 0.0006\n",
            "Epoch [3/5], Batch [5000/6250], Loss: 0.0006\n",
            "Epoch [3/5], Batch [5500/6250], Loss: 0.0006\n",
            "Epoch [3/5], Batch [6000/6250], Loss: 0.0007\n",
            "Epoch [4/5], Batch [0/6250], Loss: 0.0004\n",
            "Epoch [4/5], Batch [500/6250], Loss: 0.0005\n",
            "Epoch [4/5], Batch [1000/6250], Loss: 0.0004\n",
            "Epoch [4/5], Batch [1500/6250], Loss: 0.0006\n",
            "Epoch [4/5], Batch [2000/6250], Loss: 0.0005\n",
            "Epoch [4/5], Batch [2500/6250], Loss: 0.0003\n",
            "Epoch [4/5], Batch [3000/6250], Loss: 0.0004\n",
            "Epoch [4/5], Batch [3500/6250], Loss: 0.0005\n",
            "Epoch [4/5], Batch [4000/6250], Loss: 0.0004\n",
            "Epoch [4/5], Batch [4500/6250], Loss: 0.0003\n",
            "Epoch [4/5], Batch [5000/6250], Loss: 0.0003\n",
            "Epoch [4/5], Batch [5500/6250], Loss: 0.0006\n",
            "Epoch [4/5], Batch [6000/6250], Loss: 0.0011\n",
            "Epoch [5/5], Batch [0/6250], Loss: 0.0004\n",
            "Epoch [5/5], Batch [500/6250], Loss: 0.0003\n",
            "Epoch [5/5], Batch [1000/6250], Loss: 0.0005\n",
            "Epoch [5/5], Batch [1500/6250], Loss: 0.0006\n",
            "Epoch [5/5], Batch [2000/6250], Loss: 0.0007\n",
            "Epoch [5/5], Batch [2500/6250], Loss: 0.0004\n",
            "Epoch [5/5], Batch [3000/6250], Loss: 0.0003\n",
            "Epoch [5/5], Batch [3500/6250], Loss: 0.0004\n",
            "Epoch [5/5], Batch [4000/6250], Loss: 0.0001\n",
            "Epoch [5/5], Batch [4500/6250], Loss: 0.0003\n",
            "Epoch [5/5], Batch [5000/6250], Loss: 0.0003\n",
            "Epoch [5/5], Batch [5500/6250], Loss: 0.0002\n",
            "Epoch [5/5], Batch [6000/6250], Loss: 0.0007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "doVP09z_Yx9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FYEK7eKRTRO7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}